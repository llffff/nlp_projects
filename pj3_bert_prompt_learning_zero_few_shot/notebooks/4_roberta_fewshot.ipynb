{"cells":[{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["import os\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\" # cuda:0, GPU1\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,6\" # cuda:0, GPU1\n","from time import time\n","\n","import dataset\n","from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import transformers\n","from  torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["class PromptConfig():\n","    def __init__(self, few_shot, BERT_PATH=\"bert-base-uncased\", ) -> None:\n","        \"\"\"\n","        few_shot=['0', '32', '64', '128']\n","        \"\"\"\n","        self.few_shot = few_shot\n","        self.DEVICE = \"cuda:1\"\n","        self.MAX_LEN = 256\n","        self.TRAIN_BATCH_SIZE = 8\n","        self.VALID_BATCH_SIZE = 4\n","        self.train_times = 2\n","        self.EPOCHS = 5*self.train_times\n","        \n","        self.EARLY_STOP = 3\n","        self.eval_zero_shot = True # 是否测试zero shot\n","        \n","        # 训练参数\n","        self.eps_thres=1e-4 \n","        self.es_max=5  # early stop\n","\n","        self.BERT_PATH = BERT_PATH\n","        self.MODEL_PATH = \"/home/18307110500/pj3_workplace/pytorch_model.bin\"\n","        data_dir =\"/home/18307110500/data\"\n","\n","        if few_shot is not None and few_shot == '0':\n","            self.TRAINING_FILE = None\n","        elif few_shot is not None and os.path.exists(f\"{data_dir}/train_{few_shot}.data\"):\n","            self.TRAINING_FILE =f\"{data_dir}/train_{few_shot}.data\"\n","        else:\n","            self.TRAINING_FILE = f\"{data_dir}/train.data\"\n","\n","        self.VALIDATION_FILE = f\"{data_dir}/valid.data\"\n","        self.TEST_FILE = f\"{data_dir}/test.data\"\n","\n","        BERT_PATH:str\n","        if BERT_PATH.startswith(\"bert\"):\n","            self.TOKENIZER= transformers.BertTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.BertForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        elif BERT_PATH.startswith(\"albert\"):\n","            # AlbertTokenizer, AlbertForMaskedLM\n","            self.TOKENIZER= transformers.AlbertTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.AlbertForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        elif BERT_PATH.startswith(\"roberta\"):\n","            # RobertaTokenizer, RobertaForMaskedLM\n","            self.TOKENIZER= transformers.RobertaTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.RobertaForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        # prompt\n","        # label转换为id\n","        self.mask = self.TOKENIZER.mask_token # '[MASK]'/'<mask>'\n","        # self.verbalizer=['negative', 'positive']\n","        self.verbalizer=['bad', 'great']\n","        self.template =  \"It is a {} film .\"\n","        # self.template =  \"It was {} .\" # .format('[MASK]')\n","        # self.verbalizer=['terrible', 'great']\n","        self.candidate_ids = [self.TOKENIZER._convert_token_to_id(_) for _ in self.verbalizer]\n","        \n","''' 1  '''\n","''' 基于BertMaskedML的few shot '''\n","paths= [\"bert-base-uncased\",\"bert-large-uncased\", \"albert-base-v2\", \"albert-large-v2\", \"roberta-base\",\"roberta-large\"]\n","config = PromptConfig(BERT_PATH=paths[4], few_shot=\"32\") # few shot\n","# config = PromptConfig(BERT_PATH=paths[0], few_shot=\"32\") # few shot"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# samples: 32\n","# samples: 1000\n","better than the tepid star trek : insurrection ; falls short of first contact because the villain could n't pick the lint off borg queen alice krige 's cape ; and finishes half a parsec ( a nose ) ahead of generations . It is a <mask> film . 1\n","nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic . It is a <mask> film . 0\n"]}],"source":["# model: bert masked lm\n","model_bert = config.MODEL \n","bert_tokenzier = config.TOKENIZER\n","\n","bert_tokenzier: transformers.BertTokenizer\n","\n","class PromptDataset(dataset.BERTDataset):\n","    def __init__(self, review, target, config):\n","        super(PromptDataset, self).__init__(review, target, config)\n","\n","        self.template = config.template # \"It is a {} film.\" # [MASK]\n","        self.mask = config.mask# '[MASK]' # bert_tokenzier.mask_token\n","        \n","    # sep = bert_tokenzier.sep_token\n","    def make_prompt(self, input_data):\n","        input_trans = f\"{input_data} {self.template.format(self.mask)}\"\n","        return input_trans\n","\n","    def getReview(self, item):\n","        review = super().getReview(item)\n","        review_trans = self.make_prompt(review)\n","        return review_trans\n","        \n","\n","_, train_dir= dataset.read_data(config.TRAINING_FILE)\n","_, valid_dir= dataset.read_data(config.VALIDATION_FILE)\n","\n","train_dataset = PromptDataset(train_dir['x'], train_dir['y'],config=config)\n","valid_dataset = PromptDataset(valid_dir['x'], valid_dir['y'],config=config)\n","\n","valid_data_loader = valid_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","train_data_loader = train_dataset.get_dataloader(batch_size=config.TRAIN_BATCH_SIZE)\n","\n","print(train_dataset.getReview(0), train_dataset.target[0])\n","print(valid_dataset.getReview(0), valid_dataset.target[0])\n","# \"nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic .  It is a [MASK] review.\" 0"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def get_logits_of_mask(input_ids, logits, tok=config.mask, tokenzier=bert_tokenzier):\n","    \"\"\"\n","    Args:\n","        inputs_tok (tensor): 输入字符串经过tokenized得到的字典\n","        tok (str, optional): 可以是'[MASK]'或任意word token. Defaults to '[MASK]'.\n","\n","    Returns:\n","        (tensor, tensor): 返回mask处的logits，返回mask的列索引\n","\n","    Tips: 可以传入多个batch size\n","\n","    Modify: 改为torch实现\n","\n","    \"\"\"\n","    # find_idx_of_tok_in_seq\n","    tok_id =  tokenzier._convert_token_to_id(tok)\n","    ids_of_mask_in_seq = torch.nonzero(input_ids == tok_id)[:,1] ## 得到mask的列索引\n","    \n","    # convert to tensor\n","    logits_tok = torch.stack([logits[idx, ids_of_mask_in_seq[idx],:]for idx in range(logits.size(0))])\n","\n","    # logits_tok.size() # [4, 30522]=[batch size, vocab size]\n","    return logits_tok, ids_of_mask_in_seq\n","\n","\n","''' train: fine tune bert '''\n","\n","def count_acc(pred, target):\n","    acc_count = np.sum(np.array(pred) == np.array(target))\n","    return acc_count/len(pred)\n","\n","def loss_fn(outputs, targets):\n","    # sigmoid + cross entropy\n","    # print(outputs, targets)\n","    return nn.BCEWithLogitsLoss()(outputs.view(-1,1), targets.view(-1, 1))\n","\n","\n","def get_logits(config, logits_mask):\n","    # init: candidate_ids = config.candidate_ids\n","    labels_pr = logits_mask[:, config.candidate_ids]\n","    return labels_pr"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def eval_prompt(data_loader, model, device):\n","    _targets = []\n","    _outputs = []\n","    _logits = []\n","    _mask_ids = []\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            \n","            input_token = {\n","                k[1]: d[k[0]].to(device) \n","                for k in dict_keys}\n","\n","            res_batch = model(**input_token)\n","            \n","            logits = res_batch.logits\n","            logits = logits.cpu()\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok=config.mask ,tokenzier=bert_tokenzier)\n","\n","            # optional: 计算整个vocab上的softmax score\n","            # logits_mask = torch.softmax(logits_mask, dim=-1)\n","            \n","            # 比较哪个label的mask填空可能性更大\n","            # labels_pr = logits_mask[:, config.candidate_ids]\n","            # pred = [np.argmax(_) for _ in labels_pr]\n","            labels_pr = get_logits(config,logits_mask)\n","            pred = [np.argmax(_) for _ in labels_pr]\n","                \n","            _targets.extend(d['targets'])\n","            _outputs.extend(pred)\n","            _logits.extend(logits_mask)\n","            _mask_ids.extend(mask_ids)\n","            torch.cuda.empty_cache()\n","            # break\n","    return _targets,_outputs,_logits,_mask_ids"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 250/250 [00:50<00:00,  4.97it/s]"]},{"name":"stdout","output_type":"stream","text":["[Zero shot]\n","[Eval] Acc:0.796 | pred.sum: 604 | target.sum: 510.0\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["device = config.DEVICE\n","model_bert.to(device)\n","\n","# zero\n","if config.eval_zero_shot:\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    \n","    print(\"[Zero shot]\")\n","    print(f\"[Eval] Acc:{count_acc(fin_outputs_eval,fin_targets_eval)} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["opt param:  77\n","no opt 125\n"]}],"source":["param_optimizer = list(model_bert.named_parameters())\n","no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","# for _,__ in param_optimizer:\n","#     print(_)\n","optimizer_parameters = [ {\n","        \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.001,\n","    },\n","        {\"params\": [ p for n, p in param_optimizer if any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","optimizer_parameters_name = [ {\n","        \"params\": [n for n, p in param_optimizer if not any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.001,\n","    },{\"params\": [ n for n, p in param_optimizer if any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","print(\"opt param: \",len(optimizer_parameters[0]['params']))\n","print(\"no opt\",len(optimizer_parameters[1]['params']))"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["num_train_steps = int(len(train_dir['x']) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n","if len(optimizer_parameters[0]['params']) > 20:\n","    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","else:\n","    # albert\n","    optimizer = AdamW(model_bert.parameters(), lr=3e-5)\n","    \n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",")"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["ev_acc_his = []\n","tr_loss_his = []\n","tr_time_his=[]\n","early_stop_count = 0"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:06<00:00,  1.62s/it]\n","100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n","100%|██████████| 250/250 [00:50<00:00,  4.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 0/10 | Train Loss: 0.6954 | Train time: 6.843363523483276s\n","[Eval] Acc:0.847 | pred.sum: 407 | target.sum: 510.0\n","[Best epoch]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:06<00:00,  1.72s/it]\n","100%|██████████| 4/4 [00:06<00:00,  1.69s/it]\n","100%|██████████| 250/250 [00:51<00:00,  4.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 1/10 | Train Loss: 0.5927 | Train time: 7.219158291816711s\n","[Eval] Acc:0.845 | pred.sum: 467 | target.sum: 510.0\n","[Best epoch]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:07<00:00,  1.91s/it]\n","100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n","100%|██████████| 250/250 [00:51<00:00,  4.83it/s]"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 2/10 | Train Loss: 0.5953 | Train time: 7.849446415901184s\n","[Eval] Acc:0.721 | pred.sum: 239 | target.sum: 510.0\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 4/4 [00:06<00:00,  1.63s/it]\n","100%|██████████| 4/4 [00:06<00:00,  1.65s/it]\n","100%|██████████| 250/250 [00:52<00:00,  4.77it/s]"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 3/10 | Train Loss: 0.5933 | Train time: 6.930572152137756s\n","[Eval] Acc:0.781 | pred.sum: 317 | target.sum: 510.0\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 4/4 [00:06<00:00,  1.62s/it]\n","100%|██████████| 4/4 [00:06<00:00,  1.72s/it]\n","100%|██████████| 250/250 [00:53<00:00,  4.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 4/10 | Train Loss: 0.5452 | Train time: 7.056839942932129s\n","[Eval] Acc:0.831 | pred.sum: 409 | target.sum: 510.0\n"]}],"source":["# test\n","# config.train_times = 1\n","# config.EPOCHS=5\n","# test\n","\n","for epoch in range(config.EPOCHS//config.train_times):\n","        \n","    # begin training\n","    model_bert.train()\n","    tr_time_s = time()\n","\n","    tr_loss = []\n","\n","    # config.train_times = 5\n","    for epo_tr in range(config.train_times):\n","        for bi, d in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            targets = d['targets']\n","            input_token = {\n","                    k[1]: d[k[0]].to(device) \n","                    for k in dict_keys}\n","\n","            optimizer.zero_grad()\n","            res_batch = model_bert(**input_token)\n","            logits = res_batch.logits.cpu()\n","\n","            ''' 取出mask位置上，candidate label对应的logits '''\n","            # mask 位置的预测logits\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok=config.mask,tokenzier=bert_tokenzier)\n","            # logits_mask: (batch_size, vocab_size)\n","            \n","            # mask位置预测为candidate label的概率分数\n","            labels_pr = get_logits(config, logits_mask)\n","            labels_pr = torch.softmax(labels_pr, dim=-1)\n","            \n","            # labels_pr: (batch_size, 2)\n","\n","            # 取出 positive 对应的分数 (negative = 1-positive)\n","            pred = labels_pr[:,1]\n","            loss = loss_fn(pred, targets)\n","            tr_loss.append(loss.cpu().detach().item())\n","            # print(loss) # 0.6433\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            torch.cuda.empty_cache()\n","\n","    tr_time_his.append((time()-tr_time_s)/config.train_times)\n","    tr_loss_his.append(np.mean(tr_loss))\n","\n","    # begin eval\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    \n","    ev_acc_his.append(count_acc(fin_outputs_eval,fin_targets_eval))\n","\n","    loss_str = \"{:.4f}\".format(tr_loss_his[-1])\n","    print(f\"[Train] Epoch: {epoch}/{config.EPOCHS} | Train Loss: {loss_str} | Train time: {tr_time_his[-1]}s\")\n","    print(f\"[Eval] Acc:{ev_acc_his[-1]} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")\n","\n","    best_acc = max(ev_acc_his[:-1]) if epoch > 1 else -10\n","    if ev_acc_his[-1] > best_acc: # > best acc\n","        torch.save(model_bert, f\"fewshot{config.few_shot}-{config.BERT_PATH}-best.pth\")\n","        print(\"[Best epoch]\")\n","        # reset\n","        early_stop_count= 0\n","    if early_stop_count > config.EARLY_STOP: \n","        print(f\"[WARNING] early stop at epoch {epoch}.\")\n","        break"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["def show_topk_cloze(logits_mask,top_k =10):\n","    # 根据logits排序\n","    top_inds = list(reversed(np.argsort(logits_mask)))\n","    res_top_k = []\n","    for i in top_inds:\n","        res_i = {\n","            \"token_id\":i.item(),\n","            \"token_str\": bert_tokenzier._convert_id_to_token(i.item()),\n","            \"raw_score\": logits_mask[i.item()] # 未经过softmax的分数\n","            }\n","        res_top_k.append(res_i)\n","        if len(res_top_k) >= top_k:\n","            break\n","\n","    return res_top_k # 查看top k预测的填空"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["sequence: 'i was feeling this movie until it veered off too far into the exxon zone , and left me behind at the station looking for a return ticket to realism . It is a <mask> film .', target: 0, pred: 0 [{'token_id': 1099, 'token_str': 'Ġbad', 'raw_score': tensor(17.5217)}, {'token_id': 2430, 'token_str': 'Ġnegative', 'raw_score': tensor(15.0346)}, {'token_id': 6587, 'token_str': 'Ġterrible', 'raw_score': tensor(14.2009)}, {'token_id': 685, 'token_str': 'Ġlost', 'raw_score': tensor(13.5800)}, {'token_id': 5074, 'token_str': 'Ġsad', 'raw_score': tensor(13.2272)}, {'token_id': 11385, 'token_str': 'Ġhorrible', 'raw_score': tensor(13.0578)}, {'token_id': 6770, 'token_str': 'Ġdisappointing', 'raw_score': tensor(13.0543)}, {'token_id': 1593, 'token_str': 'Ġwrong', 'raw_score': tensor(13.0152)}, {'token_id': 2933, 'token_str': 'Ġdark', 'raw_score': tensor(12.7854)}, {'token_id': 2129, 'token_str': 'Ġpoor', 'raw_score': tensor(12.6663)}]\n"]}],"source":["idx = 100\n","if idx >= 0:\n","    logits = fin_logits_eval[idx]\n","    pred = fin_outputs_eval[idx]\n","\n","    target = valid_dataset.target[idx]\n","    sequence = valid_dataset.getReview(idx)\n","    ids = valid_dataset[idx]['ids']\n","    print(f\"sequence: \\'{sequence}\\', target: {target}, pred: {pred}\", show_topk_cloze(logits, top_k=10))"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# samples: 1066\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 267/267 [00:55<00:00,  4.83it/s]"]},{"name":"stdout","output_type":"stream","text":["Testing finish. Test results saved.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# 1. 测试\n","eval_acc = max(ev_acc_his) # best eval\n","model = torch.load(f\"fewshot{config.few_shot}-{config.BERT_PATH}-best.pth\")\n","\n","_, test_dir= dataset.read_data(config.TEST_FILE, test=True)\n","test_dataset = PromptDataset(test_dir['x'], test_dir['y'],config=config)\n","test_data_loader = test_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","\n","test_record = eval_prompt(test_data_loader, model, device)\n","# targets ,outputs ,logits ,mask_ids\n","test_preds = test_record[1]\n","\n","# 2. open文件写入结果\n","with open(f'saved/few_shot{config.few_shot}_eval{eval_acc}_res.txt',encoding=\"utf-8\", mode='w') as f:\n","    for pred in test_preds:\n","        f.write(\"positive\" if pred==1 else 'negative')\n","        f.write('\\n')\n","print(\"Testing finish. Test results saved.\")"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epo</th>\n","      <th>eval acc</th>\n","      <th>train loss</th>\n","      <th>epoch time(s)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>0.847</td>\n","      <td>0.695414</td>\n","      <td>6.843364</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>0.845</td>\n","      <td>0.592689</td>\n","      <td>7.219158</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6</td>\n","      <td>0.721</td>\n","      <td>0.595258</td>\n","      <td>7.849446</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>8</td>\n","      <td>0.781</td>\n","      <td>0.593274</td>\n","      <td>6.930572</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10</td>\n","      <td>0.831</td>\n","      <td>0.545173</td>\n","      <td>7.056840</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   epo  eval acc  train loss  epoch time(s)\n","0    2     0.847    0.695414       6.843364\n","1    4     0.845    0.592689       7.219158\n","2    6     0.721    0.595258       7.849446\n","3    8     0.781    0.593274       6.930572\n","4   10     0.831    0.545173       7.056840"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["metric_rec = {\n","    'epo':[(i+1)*config.train_times for i in range(len(ev_acc_his))],\n","    'eval acc': ev_acc_his,\n","    'train loss': tr_loss_his ,\n","    'epoch time(s)': tr_time_his\n","}\n","data_f = pd.DataFrame(metric_rec)\n","data_f"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["model roberta-base | fewshot 32 | best acc 0.847 | epoch 10 | 7.180s\n"]}],"source":["avg_epo_time= np.average(tr_time_his)\n","print(\"model {} | fewshot {} | best acc {} | epoch {} | {:.3f}s\".format(config.BERT_PATH ,config.few_shot,eval_acc, config.EPOCHS,avg_epo_time))"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3 (default, May 19 2020, 18:47:26) \n[GCC 7.3.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}},"nbformat":4,"nbformat_minor":2}
