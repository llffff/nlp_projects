{"cells":[{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import os\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\" # cuda:0, GPU1\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" # cuda:0, GPU1\n","from time import time\n","\n","import dataset\n","from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import transformers\n","from  torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class PromptConfig():\n","    def __init__(self, few_shot, BERT_PATH=\"bert-base-uncased\", ) -> None:\n","        \"\"\"\n","        few_shot=['0', '32', '64', '128']\n","        \"\"\"\n","        self.few_shot = few_shot\n","        self.DEVICE = \"cuda:0\"\n","        self.MAX_LEN = 256\n","        self.TRAIN_BATCH_SIZE = 8\n","        self.VALID_BATCH_SIZE = 4\n","        self.EPOCHS = 10\n","        \n","        self.EARLY_STOP = 3\n","        \n","        self.eval_zero_shot = False # 是否测试zero shot\n","        \n","        # 训练参数\n","        self.eps_thres=1e-4 \n","        self.es_max=5  # early stop\n","\n","        self.BERT_PATH = BERT_PATH\n","        self.MODEL_PATH = \"/home/18307110500/pj3_workplace/pytorch_model.bin\"\n","        data_dir =\"/home/18307110500/data\"\n","\n","        if few_shot is not None and few_shot == '0':\n","            self.TRAINING_FILE = None\n","        elif few_shot is not None and os.path.exists(f\"{data_dir}/train_{few_shot}.data\"):\n","            self.TRAINING_FILE =f\"{data_dir}/train_{few_shot}.data\"\n","        else:\n","            self.TRAINING_FILE = f\"{data_dir}/train.data\"\n","\n","        self.VALIDATION_FILE = f\"{data_dir}/valid.data\"\n","        self.TEST_FILE = f\"{data_dir}/test.data\"\n","        self.TOKENIZER= transformers.BertTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","\n","        # prompt\n","        # label转换为id\n","        # self.verbalizer=['negative', 'positive']\n","        self.verbalizer=['bad', 'great']\n","        self.candidate_ids = [self.TOKENIZER._convert_token_to_id(_) for _ in self.verbalizer]\n","\n","        self.template =  \"It is a {} film.\" # .format('[MASK]')\n","        self.mask = self.TOKENIZER.mask_token # '[MASK]'\n","        \n","''' 1  '''\n","''' 基于BertMaskedML的few shot '''\n","config = PromptConfig(BERT_PATH=\"bert-base-uncased\", few_shot=\"32\") # few shot"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["# model: bert masked lm\n","model_bert = transformers.BertForMaskedLM.from_pretrained(config.BERT_PATH)\n","bert_tokenzier = config.TOKENIZER\n","\n","bert_tokenzier: transformers.BertTokenizer"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# samples: 32\n","# samples: 1000\n","better than the tepid star trek : insurrection ; falls short of first contact because the villain could n't pick the lint off borg queen alice krige 's cape ; and finishes half a parsec ( a nose ) ahead of generations . It is a [MASK] film. 1\n","nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic . It is a [MASK] film. 0\n"]}],"source":["class PromptDataset(dataset.BERTDataset):\n","    def __init__(self, review, target, config):\n","        super(PromptDataset, self).__init__(review, target, config)\n","\n","        self.template = config.template # \"It is a {} film.\" # [MASK]\n","        self.mask = config.mask# '[MASK]' # bert_tokenzier.mask_token\n","        \n","    # sep = bert_tokenzier.sep_token\n","    def make_prompt(self, input_data):\n","        input_trans = f\"{input_data} {self.template.format(self.mask)}\"\n","        return input_trans\n","\n","    def getReview(self, item):\n","        review = super().getReview(item)\n","        review_trans = self.make_prompt(review)\n","        return review_trans\n","        \n","\n","_, train_dir= dataset.read_data(config.TRAINING_FILE)\n","_, valid_dir= dataset.read_data(config.VALIDATION_FILE)\n","\n","train_dataset = PromptDataset(train_dir['x'], train_dir['y'],config=config)\n","valid_dataset = PromptDataset(valid_dir['x'], valid_dir['y'],config=config)\n","\n","valid_data_loader = valid_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","train_data_loader = train_dataset.get_dataloader(batch_size=config.TRAIN_BATCH_SIZE)\n","\n","print(train_dataset.getReview(0), train_dataset.target[0])\n","print(valid_dataset.getReview(0), valid_dataset.target[0])\n","# \"nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic .  It is a [MASK] review.\" 0"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n","100%|██████████| 250/250 [00:15<00:00, 15.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 0/10 | Train Loss: 0.6917 | Train time: 4.176789045333862s\n","[Eval] Acc:0.729 | pred.sum: 439 | target.sum: 510.0\n","[Best epoch]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:04<00:00,  1.01s/it]\n","100%|██████████| 250/250 [00:32<00:00,  7.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 1/10 | Train Loss: 0.6378 | Train time: 4.319622278213501s\n","[Eval] Acc:0.748 | pred.sum: 422 | target.sum: 510.0\n","[Best epoch]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:03<00:00,  1.01it/s]\n","100%|██████████| 250/250 [00:33<00:00,  7.57it/s]"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 2/10 | Train Loss: 0.5565 | Train time: 4.21187686920166s\n","[Eval] Acc:0.722 | pred.sum: 272 | target.sum: 510.0\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 4/4 [00:03<00:00,  1.11it/s]\n","100%|██████████| 250/250 [00:32<00:00,  7.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 3/10 | Train Loss: 0.5233 | Train time: 3.8851301670074463s\n","[Eval] Acc:0.776 | pred.sum: 470 | target.sum: 510.0\n","[Best epoch]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:03<00:00,  1.13it/s]\n","100%|██████████| 250/250 [00:33<00:00,  7.54it/s]"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 4/10 | Train Loss: 0.5063 | Train time: 3.763291597366333s\n","[Eval] Acc:0.775 | pred.sum: 587 | target.sum: 510.0\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 4/4 [00:03<00:00,  1.11it/s]\n","100%|██████████| 250/250 [00:33<00:00,  7.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 5/10 | Train Loss: 0.5124 | Train time: 3.8192882537841797s\n","[Eval] Acc:0.792 | pred.sum: 554 | target.sum: 510.0\n","[Best epoch]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:03<00:00,  1.13it/s]\n","100%|██████████| 250/250 [00:33<00:00,  7.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 6/10 | Train Loss: 0.5033 | Train time: 3.799058675765991s\n","[Eval] Acc:0.794 | pred.sum: 514 | target.sum: 510.0\n","[Best epoch]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:03<00:00,  1.12it/s]\n","100%|██████████| 250/250 [00:32<00:00,  7.58it/s]"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 7/10 | Train Loss: 0.5051 | Train time: 3.8150627613067627s\n","[Eval] Acc:0.792 | pred.sum: 492 | target.sum: 510.0\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 4/4 [00:03<00:00,  1.09it/s]\n","100%|██████████| 250/250 [00:33<00:00,  7.52it/s]"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 8/10 | Train Loss: 0.5033 | Train time: 3.8889896869659424s\n","[Eval] Acc:0.794 | pred.sum: 488 | target.sum: 510.0\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 4/4 [00:03<00:00,  1.08it/s]\n","100%|██████████| 250/250 [00:32<00:00,  7.59it/s]"]},{"name":"stdout","output_type":"stream","text":["[Train] Epoch: 9/10 | Train Loss: 0.5034 | Train time: 3.9282917976379395s\n","[Eval] Acc:0.794 | pred.sum: 486 | target.sum: 510.0\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["def get_logits_of_mask(input_ids,logits, tok='[MASK]', tokenzier=bert_tokenzier):\n","    \"\"\"\n","    Args:\n","        inputs_tok (tensor): 输入字符串经过tokenized得到的字典\n","        tok (str, optional): 可以是'[MASK]'或任意word token. Defaults to '[MASK]'.\n","\n","    Returns:\n","        (tensor, tensor): 返回mask处的logits，返回mask的列索引\n","\n","    Tips: 可以传入多个batch size\n","\n","    Modify: 改为torch实现\n","    \"\"\"\n","\n","    # find_idx_of_tok_in_seq\n","    tok_id =  tokenzier._convert_token_to_id(tok)\n","    ids_of_mask_in_seq = torch.nonzero(input_ids == tok_id)[:,1] ## 得到mask的列索引\n","    \n","    # convert to tensor\n","    logits_tok = torch.stack([logits[idx, ids_of_mask_in_seq[idx],:]for idx in range(logits.size(0))])\n","\n","    # logits_tok.size() # [4, 30522]=[batch size, vocab size]\n","    return logits_tok, ids_of_mask_in_seq\n","\n","\n","device = config.DEVICE\n","model_bert.to(device)\n","\n","''' train: fine tune bert '''\n","\n","def count_acc(pred, target):\n","    acc_count = np.sum(np.array(pred)== np.array(target))\n","    return acc_count/len(pred)\n","\n","def loss_fn(outputs, targets):\n","    # sigmoid + cross entropy\n","    # print(outputs, targets)\n","    return nn.BCEWithLogitsLoss()(outputs.view(-1,1), targets.view(-1, 1))\n","\n","param_optimizer = list(model_bert.named_parameters())\n","no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","optimizer_parameters = [ {\n","        \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.001,\n","    },{\"params\": [ p for n, p in param_optimizer if any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","num_train_steps = int(len(train_dir['x']) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n","optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",")\n","\n","def eval_prompt(data_loader, model, device):\n","    _targets = []\n","    _outputs = []\n","    _logits = []\n","    _mask_ids = []\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            \n","            input_token = {\n","                k[1]: d[k[0]].to(device) \n","                for k in dict_keys}\n","\n","            res_batch = model(**input_token)\n","            \n","            logits = res_batch.logits\n","            logits = logits.cpu()\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok='[MASK]',tokenzier=bert_tokenzier)\n","\n","            # optional: 计算整个vocab上的softmax score\n","            # logits_mask = torch.softmax(logits_mask, dim=-1)\n","            \n","            # 比较哪个label的mask填空可能性更大\n","            labels_pr = logits_mask[:, config.candidate_ids]\n","            pred = [np.argmax(_) for _ in labels_pr]\n","                \n","            _targets.extend(d['targets'])\n","            _outputs.extend(pred)\n","            _logits.extend(logits_mask)\n","            _mask_ids.extend(mask_ids)\n","            torch.cuda.empty_cache()\n","            # break\n","    return _targets,_outputs,_logits,_mask_ids\n","\n","\n","# zero\n","if config.eval_zero_shot:\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    \n","    print(\"[Zero shot]\")\n","    print(f\"[Eval] Acc:{count_acc(fin_outputs_eval,fin_targets_eval)} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")\n","\n","ev_acc_his = []\n","tr_loss_his = []\n","tr_time_his=[]\n","early_stop_count = 0\n","\n","for epoch in range(config.EPOCHS):\n","        \n","    # begin training\n","    model_bert.train()\n","    tr_time_s = time()\n","\n","    tr_loss = []\n","        \n","    for bi, d in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n","        dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","        targets = d['targets']\n","        input_token = {\n","                k[1]: d[k[0]].to(device) \n","                for k in dict_keys}\n","\n","        optimizer.zero_grad()\n","        res_batch = model_bert(**input_token)\n","        logits = res_batch.logits.cpu()\n","\n","        ''' 取出mask位置上，candidate label对应的logits '''\n","        # mask 位置的预测logits\n","        logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok='[MASK]',tokenzier=bert_tokenzier)\n","        # logits_mask: (batch_size, vocab_size)\n","        \n","        # mask位置预测为candidate label的概率分数\n","        labels_pr = logits_mask[:, config.candidate_ids]\n","        labels_pr = torch.softmax(labels_pr, dim=-1)\n","        # labels_pr: (batch_size, 2)\n","\n","        # 取出 positive 对应的分数 (negative = 1-positive)\n","        loss = loss_fn(labels_pr[:,1], targets)\n","        tr_loss.append(loss.cpu().detach().item())\n","        # print(loss) # 0.6433\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    tr_time_his.append(time()-tr_time_s)\n","    tr_loss_his.append(np.mean(tr_loss))\n","    torch.cuda.empty_cache()\n","\n","    # begin eval\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    \n","    ev_acc_his.append(count_acc(fin_outputs_eval,fin_targets_eval))\n","\n","    loss_str = \"{:.4f}\".format(tr_loss_his[-1])\n","    print(f\"[Train] Epoch: {epoch}/{config.EPOCHS} | Train Loss: {loss_str} | Train time: {tr_time_his[-1]}s\")\n","    print(f\"[Eval] Acc:{ev_acc_his[-1]} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")\n","\n","    best_acc = max(ev_acc_his[:-1]) if epoch > 1 else -10\n","    if ev_acc_his[-1] > best_acc: # > best acc\n","        torch.save(model_bert, f\"fewshot{config.few_shot}-best.pth\")\n","        print(\"[Best epoch]\")\n","        # reset\n","        early_stop_count= 0\n","    if early_stop_count > config.EARLY_STOP: \n","        print(f\"[WARNING] early stop at epoch {epoch}.\")\n","        break"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def show_topk_cloze(logits_mask,top_k =10):\n","    \n","    # 根据logits排序\n","    top_inds = list(reversed(np.argsort(logits_mask)))\n","    res_top_k = []\n","    for i in top_inds:\n","        res_i = {\n","            \"token_id\":i.item(),\n","            \"token_str\": bert_tokenzier._convert_id_to_token(i.item()),\n","            \"raw_score\": logits_mask[i.item()] # 未经过softmax的分数\n","            }\n","        res_top_k.append(res_i)\n","        if len(res_top_k) >= top_k:\n","            break\n","\n","    return res_top_k # 查看top k预测的填空"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["idx = -1\n","if idx >= 0:\n","    logits = fin_logits_eval[idx]\n","    pred = fin_outputs_eval[idx]\n","\n","    target = valid_dataset.target[idx]\n","    sequence = valid_dataset.getReview(idx)\n","    ids = valid_dataset[idx]['ids']\n","    (f\"sequence: \\'{sequence}\\', target: {target}, pred: {pred}\", show_topk_cloze(logits, top_k=40))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# samples: 1066\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 267/267 [00:34<00:00,  7.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Testing finish. Test results saved.\n"]}],"source":["# 1. 测试\n","eval_acc = max(ev_acc_his) # best eval\n","model = torch.load(f\"fewshot{config.few_shot}-best.pth\")\n","\n","_, test_dir= dataset.read_data(config.TEST_FILE, test=True)\n","test_dataset = PromptDataset(test_dir['x'], test_dir['y'],config=config)\n","test_data_loader = test_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","\n","test_record = eval_prompt(test_data_loader, model, device)\n","# targets ,outputs ,logits ,mask_ids\n","test_preds = test_record[1]\n","\n","# 2. open文件写入结果\n","with open(f'saved/few_shot{config.few_shot}_eval{eval_acc}_res.txt',encoding=\"utf-8\", mode='w') as f:\n","    for pred in test_preds:\n","        f.write(\"positive\" if pred==1 else 'negative')\n","        f.write('\\n')\n","print(\"Testing finish. Test results saved.\")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>eval acc</th>\n","      <th>train loss</th>\n","      <th>epoch time(s)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.729</td>\n","      <td>0.691716</td>\n","      <td>4.176789</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.748</td>\n","      <td>0.637807</td>\n","      <td>4.319622</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.722</td>\n","      <td>0.556462</td>\n","      <td>4.211877</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.776</td>\n","      <td>0.523263</td>\n","      <td>3.885130</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.775</td>\n","      <td>0.506270</td>\n","      <td>3.763292</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.792</td>\n","      <td>0.512378</td>\n","      <td>3.819288</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.794</td>\n","      <td>0.503340</td>\n","      <td>3.799059</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.792</td>\n","      <td>0.505122</td>\n","      <td>3.815063</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.794</td>\n","      <td>0.503322</td>\n","      <td>3.888990</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.794</td>\n","      <td>0.503370</td>\n","      <td>3.928292</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   eval acc  train loss  epoch time(s)\n","0     0.729    0.691716       4.176789\n","1     0.748    0.637807       4.319622\n","2     0.722    0.556462       4.211877\n","3     0.776    0.523263       3.885130\n","4     0.775    0.506270       3.763292\n","5     0.792    0.512378       3.819288\n","6     0.794    0.503340       3.799059\n","7     0.792    0.505122       3.815063\n","8     0.794    0.503322       3.888990\n","9     0.794    0.503370       3.928292"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["metric_rec = {\n","    'eval acc': ev_acc_his,\n","    'train loss': tr_loss_his ,\n","    'epoch time(s)': tr_time_his\n","}\n","data_f = pd.DataFrame(metric_rec)\n","data_f"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3.961s\n"]}],"source":["avg_epo_time= np.average(tr_time_his)\n","print(\"{:.3f}s\".format(avg_epo_time))"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3 (default, May 19 2020, 18:47:26) \n[GCC 7.3.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}},"nbformat":4,"nbformat_minor":2}
