{"cells":[{"cell_type":"code","execution_count":1,"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\" \n","from time import time\n","\n","import dataset\n","from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import transformers\n","from  torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from collections import Counter"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["class PromptConfig():\n","    def __init__(self, few_shot, BERT_PATH=\"bert-base-uncased\", ) -> None:\n","        \"\"\"\n","        few_shot=['0', '32', '64', '128']\n","        \"\"\"\n","        self.few_shot = few_shot\n","        self.DEVICE = \"cuda:5\"\n","        self.MAX_LEN = 256\n","        self.TRAIN_BATCH_SIZE = 8\n","        self.VALID_BATCH_SIZE = 4\n","        self.train_times = 1\n","        self.EPOCHS = 10*self.train_times\n","        \n","        self.EARLY_STOP = 3\n","        \n","        self.eval_zero_shot = False # 是否测试zero shot\n","        self.test_output = False # 是否测试输出\n","        self.UPDATE_VERBAL = 1\n","        # =0 不更新verbalizer\n","        # >0 每多少epoch更新一次verbalizer\n","        \n","        \n","        # 训练参数\n","        self.eps_thres=1e-4 \n","        self.es_max=5  # early stop\n","\n","        self.BERT_PATH = BERT_PATH\n","        self.MODEL_PATH = \"/home/18307110500/pj3_workplace/pytorch_model.bin\"\n","        data_dir =\"/home/18307110500/data\"\n","\n","        if few_shot is not None and few_shot == '0':\n","            self.TRAINING_FILE = None\n","        elif few_shot is not None and os.path.exists(f\"{data_dir}/train_{few_shot}.data\"):\n","            self.TRAINING_FILE =f\"{data_dir}/train_{few_shot}.data\"\n","        else:\n","            self.TRAINING_FILE = f\"{data_dir}/train.data\"\n","\n","        self.VALIDATION_FILE = f\"{data_dir}/valid.data\"\n","        self.TEST_FILE = f\"{data_dir}/test.data\"\n","\n","        BERT_PATH:str\n","        if BERT_PATH.startswith(\"bert\"):\n","            self.TOKENIZER= transformers.BertTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.BertForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        elif BERT_PATH.startswith(\"albert\"):\n","            # AlbertTokenizer, AlbertForMaskedLM\n","            self.TOKENIZER= transformers.AlbertTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.AlbertForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        elif BERT_PATH.startswith(\"roberta\"):\n","            # RobertaTokenizer, RobertaForMaskedLM\n","            self.TOKENIZER= transformers.RobertaTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.RobertaForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        # prompt\n","        # label转换为id\n","        self.mask = self.TOKENIZER.mask_token # '[MASK]'/'<mask>'\n","        # self.verbalizer=['negative', 'positive']\n","        self.verbalizer=['bad', 'great']\n","        self.template =  \"It is a {} film .\"\n","        # self.template =  \"It was {} .\" # .format('[MASK]')\n","        # self.verbalizer=['terrible', 'great']\n","        self.candidate_ids = [self.TOKENIZER._convert_token_to_id(_) for _ in self.verbalizer]\n","        \n","''' 基于 BertMaskedML的 few shot '''\n","paths= [\"bert-base-uncased\",\"bert-large-uncased\", \"albert-base-v2\", \"albert-large-v2\", \"roberta-base\",\"roberta-large\"]\n","config = PromptConfig(BERT_PATH=paths[0], few_shot=\"32\") # few shot"],"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["# model: bert masked lm\n","model_bert = config.MODEL \n","bert_tokenzier = config.TOKENIZER\n","\n","bert_tokenzier: transformers.BertTokenizer\n","\n","class PromptDataset(dataset.BERTDataset):\n","    def __init__(self, review, target, config):\n","        super(PromptDataset, self).__init__(review, target, config)\n","\n","        self.template = config.template # \"It is a {} film.\" # [MASK]\n","        self.mask = config.mask# '[MASK]' # bert_tokenzier.mask_token\n","        \n","    # sep = bert_tokenzier.sep_token\n","    def make_prompt(self, input_data):\n","        input_trans = f\"{input_data} {self.template.format(self.mask)}\"\n","        return input_trans\n","\n","    def getReview(self, item):\n","        review = super().getReview(item)\n","        review_trans = self.make_prompt(review)\n","        return review_trans\n","        \n","\n","_, train_dir= dataset.read_data(config.TRAINING_FILE)\n","_, valid_dir= dataset.read_data(config.VALIDATION_FILE)\n","\n","train_dataset = PromptDataset(train_dir['x'], train_dir['y'],config=config)\n","valid_dataset = PromptDataset(valid_dir['x'], valid_dir['y'],config=config)\n","\n","valid_data_loader = valid_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","train_data_loader = train_dataset.get_dataloader(batch_size=config.TRAIN_BATCH_SIZE)\n","\n","print(train_dataset.getReview(0), train_dataset.target[0])\n","print(valid_dataset.getReview(0), valid_dataset.target[0])\n","# \"nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic .  It is a [MASK] review.\" 0"],"outputs":[{"output_type":"stream","name":"stdout","text":["# samples: 32\n","# samples: 1000\n","better than the tepid star trek : insurrection ; falls short of first contact because the villain could n't pick the lint off borg queen alice krige 's cape ; and finishes half a parsec ( a nose ) ahead of generations . It is a [MASK] film . 1\n","nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic . It is a [MASK] film . 0\n"]}],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["def get_logits_of_mask(input_ids, logits, tok=config.mask, tokenzier=bert_tokenzier):\n","    \"\"\"\n","    Args:\n","        inputs_tok (tensor): 输入字符串经过tokenized得到的字典\n","        tok (str, optional): 可以是'[MASK]'或任意word token. Defaults to '[MASK]'.\n","\n","    Returns:\n","        (tensor, tensor): 返回mask处的logits，返回mask的列索引\n","\n","    Tips: 可以传入多个batch size\n","\n","    Modify: 改为torch实现\n","\n","    \"\"\"\n","    # find_idx_of_tok_in_seq\n","    tok_id =  tokenzier._convert_token_to_id(tok)\n","    ids_of_mask_in_seq = torch.nonzero(input_ids == tok_id)[:,1] ## 得到mask的列索引\n","    \n","    # convert to tensor\n","    logits_tok = torch.stack([logits[idx, ids_of_mask_in_seq[idx],:]for idx in range(logits.size(0))])\n","\n","    # logits_tok.size() # [4, 30522]=[batch size, vocab size]\n","    return logits_tok, ids_of_mask_in_seq\n","\n","\n","''' train: fine tune bert '''\n","\n","def count_acc(pred, target):\n","    acc_count = np.sum(np.array(pred) == np.array(target))\n","    return acc_count/len(pred)\n","\n","def loss_fn(outputs, targets):\n","    # sigmoid + cross entropy\n","    # print(outputs, targets)\n","    return nn.BCEWithLogitsLoss()(outputs.view(-1,1), targets.view(-1, 1))\n","\n","\n","def get_logits(config, logits_mask):\n","    # init: candidate_ids = config.candidate_ids\n","    labels_pr = logits_mask[:, config.candidate_ids]\n","    return labels_pr\n","\n","def get_topk_token_ids(logits_mask,top_k =10):\n","    logits_mask_=logits_mask.detach()\n","    batch_size = logits_mask_.size(0)\n","    idsk = []\n","    logitsk = []\n","    \n","    for i in range(batch_size):\n","        \n","        top_inds = list(reversed(np.argsort(logits_mask_[i].numpy(), axis=-1)))  # list\n","        idsk.append(top_inds[:top_k])\n","        logitsk.append(logits_mask_[i,top_inds][:top_k].numpy().tolist())\n","        \n","    return idsk, logitsk # (list, list) size=(bz, k)\n","\n","        \n","def show_topk_cloze(logits_mask,top_k =10):\n","    # 根据logits排序\n","    top_inds = list(reversed(np.argsort(logits_mask)))\n","    res_top_k = []\n","    for i in top_inds:\n","        res_i = {\n","            \"token_id\":i.item(),\n","            \"token_str\": bert_tokenzier._convert_id_to_token(i.item()),\n","            \"raw_score\": logits_mask[i.item()] # 未经过softmax的分数\n","            }\n","        res_top_k.append(res_i)\n","        if len(res_top_k) >= top_k:\n","            break\n","\n","    return res_top_k # 查看top k预测的填空"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["def eval_prompt(data_loader, model, device):\n","    _targets = []\n","    _outputs = []\n","    _logits = []\n","    _mask_ids = []\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            \n","            input_token = {\n","                k[1]: d[k[0]].to(device) \n","                for k in dict_keys}\n","\n","            res_batch = model(**input_token)\n","            \n","            logits = res_batch.logits\n","            logits = logits.cpu()\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok=config.mask ,tokenzier=bert_tokenzier)\n","\n","            # optional: 计算整个vocab上的softmax score\n","            # logits_mask = torch.softmax(logits_mask, dim=-1)\n","            \n","            # 取出verbalizer对应的logits\n","            labels_pr = get_logits(config, logits_mask)\n","            pred = [np.argmax(_) for _ in labels_pr]\n","                \n","            _targets.extend(d['targets'].numpy().tolist())\n","            _outputs.extend(pred)\n","            _logits.extend(logits_mask)\n","            _mask_ids.extend(mask_ids)\n","            torch.cuda.empty_cache()\n","            # break\n","    return _targets,_outputs,_logits,_mask_ids"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["device = config.DEVICE\n","model_bert.to(device)\n","\n","# zero\n","if config.eval_zero_shot:\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    \n","    print(\"[Zero shot]\")\n","    print(f\"[Eval] Acc:{count_acc(fin_outputs_eval,fin_targets_eval)} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["param_optimizer = list(model_bert.named_parameters())\n","no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","# for _,__ in param_optimizer:\n","#     print(_)\n","optimizer_parameters = [ {\n","        \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.001,\n","    },\n","        {\"params\": [ p for n, p in param_optimizer if any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","\n","print(\"opt param: \",len(optimizer_parameters[0]['params']))\n","print(\"no opt\",len(optimizer_parameters[1]['params']))"],"outputs":[{"output_type":"stream","name":"stdout","text":["opt param:  76\n","no opt 126\n"]}],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["num_train_steps = int(len(train_dir['x']) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n","if len(optimizer_parameters[0]['params']) > 20:\n","    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","else:\n","    # albert\n","    optimizer = AdamW(model_bert.parameters(), lr=3e-5)\n","    \n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["ev_acc_his = []\n","tr_loss_his = []\n","tr_time_his=[]\n","\n","early_stop_count = 0\n","\n","#test\n","# config.EPOCHS=config.train_times=1\n","#test\n","        \n","for epoch in range(config.EPOCHS//config.train_times):\n","    # begin training\n","    model_bert.train()\n","    tr_time_s = time()\n","\n","    tr_loss = []\n","\n","    # config.train_times = 5\n","    for epo_tr in range(config.train_times):\n","        for bi, d in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            targets = d['targets']\n","            input_token = {\n","                    k[1]: d[k[0]].to(device) \n","                    for k in dict_keys}\n","\n","            optimizer.zero_grad()\n","            res_batch = model_bert(**input_token)\n","            logits = res_batch.logits.cpu()\n","\n","            ''' 取出mask位置上，candidate label对应的logits '''\n","            # mask 位置的预测logits\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok=config.mask,tokenzier=bert_tokenzier)\n","            # logits_mask: (batch_size, vocab_size)\n","\n","            ### 计算loss\n","            \n","            # 取出verbalizer对应的logits\n","            labels_pr = get_logits(config, logits_mask)\n","            # 概率分数\n","            labels_pr = torch.softmax(labels_pr, dim=-1)\n","            # labels_pr: (batch_size, 2)\n","            # 取出 positive 对应的分数 (negative = 1-positive)\n","            pred = labels_pr[:,1]\n","            loss = loss_fn(pred, targets)\n","            \n","            tr_loss.append(loss.cpu().detach().item())\n","            # print(loss) # 0.6433\n","\n","            ### 反传\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            torch.cuda.empty_cache()\n","\n","    tr_time_his.append((time()-tr_time_s)/config.train_times)\n","    tr_loss_his.append(np.mean(tr_loss))\n","\n","    # begin eval\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    \n","    ev_acc_his.append(count_acc(fin_outputs_eval,fin_targets_eval))\n","\n","    loss_str = \"{:.4f}\".format(tr_loss_his[-1])\n","    print(f\"[Train] Epoch: {epoch}/{config.EPOCHS} | Train Loss: {loss_str} | Train time: {tr_time_his[-1]}s\")\n","    print(f\"[Eval] Acc:{ev_acc_his[-1]} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")\n","\n","    ### 更新verbalizer\n","    # 仅计算loss，选取令loss最小的verbal，不更新梯度\n","    model_bert.eval()\n","    tars,outputs,mask_logits,mask_ids = eval_prompt(train_data_loader, model_bert, device)\n","    tars = torch.tensor(tars)\n","\n","    # mask_logits: (train size, vocab size)\n","    \n","    # 从mask logits中计算top k\n","    mask_logits = torch.stack(mask_logits)\n","    topk_ids_all, topk_score_all = get_topk_token_ids(mask_logits)\n","        \n","    # (train size, k)\n","    \n","    # 计算loss\n","    def get_new_verbels(cnt_pos:Counter, cnt_neg:Counter, find_n=5, tokenizer=config.TOKENIZER):\n","        \"\"\"\n","        返回list of {token index, token str}\n","        \"\"\"\n","        verbels_pos =cnt_pos.most_common(find_n)\n","        verbels_pos=[{\n","            'tok_id':_[0], \n","            'tok_str': tokenizer._convert_id_to_token(_[0])}\n","            for _ in verbels_pos ]\n","\n","        verbels_neg =cnt_neg.most_common(find_n)\n","        verbels_neg=[{\n","            'tok_id':_[0], \n","            'tok_str': tokenizer._convert_id_to_token(_[0])}\n","            for _ in verbels_neg]\n","        \n","        return verbels_pos, verbels_neg\n","\n","    topk_ids_all = torch.tensor(topk_ids_all)\n","    topk_score_all = torch.tensor(topk_score_all)\n","    \n","    pos_idx = tars == 1\n","    pos_idx = torch.nonzero(pos_idx)\n","    topk_ids_pos = topk_ids_all[pos_idx]\n","    topk_score_pos = topk_score_all[pos_idx]\n","    \n","    neg_idx = tars == 0\n","    neg_idx = torch.nonzero(neg_idx)\n","    topk_ids_neg = topk_ids_all[neg_idx]\n","    topk_score_neg = topk_score_all[neg_idx]\n","    \n","    cnt_pos = np.array(topk_ids_pos).reshape(-1).tolist()\n","    cnt_pos = Counter(cnt_pos)\n","    cnt_pos.most_common(3)\n","    \n","    cnt_neg = np.array(topk_ids_neg).reshape(-1).tolist()\n","    cnt_neg = Counter(cnt_neg)\n","\n","\n","    ### 1: most freq\n","    pos_v, neg_v = get_new_verbels(cnt_pos,cnt_neg)\n","    print(pos_v, neg_v)\n","    \n","    ### 计算损失\n","    def get_loss(config, logits_mask, targets):\n","        \"\"\"\n","        logits_mask: (batch_size, )\n","        \"\"\"\n","        # 取出verbalizer对应的logits\n","        labels_pr = get_logits(config, logits_mask)\n","        # 概率分数\n","        labels_pr = torch.softmax(labels_pr, dim=-1)\n","        # 取出 positive 对应的分数 (negative = 1-positive)\n","        pred = labels_pr[:,1]\n","        loss = loss_fn(pred, targets)\n","        return loss\n","    \n","\n","    def get_new_verbelizer(config, verbelizers, mask_logits, tars, pos_or_neg=\"pos\"):\n","        temp = config.candidate_ids\n","\n","        # compute loss\n","        losses = []\n","        for verbelizer_ in verbelizers:\n","            if pos_or_neg == 'pos':\n","                config.candidate_ids = [temp[0], verbelizer_['tok_id']]\n","            else:\n","                config.candidate_ids = [verbelizer_['tok_id'],temp[1]]\n","            l_ = get_loss(config, mask_logits,targets=tars)\n","            losses.append(l_.detach().item())\n","        config.candidate_ids = temp\n","\n","        # post process\n","        losses = torch.tensor(losses)\n","\n","        i = torch.argmin(losses)\n","        verbelizers[i]['loss'] = losses[i]\n","        \n","        print(pos_or_neg, verbelizers[i])\n","        tok_id_min_loss = verbelizers[i]['tok_id']\n","\n","        return tok_id_min_loss\n","    \n","        \n","    ### 2: argmin loss Z[y]\n","    new_pos_tok_id = get_new_verbelizer(config, verbelizers=pos_v, mask_logits=mask_logits,tars=tars, pos_or_neg=\"pos\")\n","    \n","    new_neg_tok_id = get_new_verbelizer(config, verbelizers=neg_v, mask_logits=mask_logits,tars=tars, pos_or_neg=\"neg\")\n","        \n","\n","\n","\n","    best_acc = max(ev_acc_his[:-1]) if epoch > 1 else -10\n","    if ev_acc_his[-1] > best_acc: # > best acc\n","        torch.save(model_bert, f\"fewshot{config.few_shot}-{config.BERT_PATH}-best.pth\")\n","        print(\"[Best epoch]\")\n","        # reset\n","        early_stop_count= 0\n","    else:\n","        early_stop_count+=1\n","        \n","    if early_stop_count >= config.EARLY_STOP: \n","        print(f\"[WARNING] early stop at epoch {epoch}.\")\n","        break\n","\n","    if config.UPDATE_VERBAL>0 and ((epoch+1) % config.UPDATE_VERBAL)==0:\n","        config.verbalizer = [bert_tokenzier._convert_id_to_token(new_neg_tok_id),bert_tokenzier._convert_id_to_token(new_pos_tok_id) ]\n","        config.candidate_ids = [new_neg_tok_id, new_pos_tok_id]\n","        print(f\"[Update verbalizer] {config.verbalizer}\")"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:07<00:00,  1.83s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.50it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 0/10 | Train Loss: 0.6453 | Train time: 7.529954433441162s\n","[Eval] Acc:0.777 | pred.sum: 367 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4/4 [00:01<00:00,  3.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[{'tok_id': 3376, 'tok_str': 'beautiful'}, {'tok_id': 10392, 'tok_str': 'fantastic'}, {'tok_id': 6919, 'tok_str': 'wonderful'}, {'tok_id': 9487, 'tok_str': 'remarkable'}, {'tok_id': 2307, 'tok_str': 'great'}] [{'tok_id': 2919, 'tok_str': 'bad'}, {'tok_id': 6659, 'tok_str': 'terrible'}, {'tok_id': 5469, 'tok_str': 'horror'}, {'tok_id': 10398, 'tok_str': 'propaganda'}, {'tok_id': 9202, 'tok_str': 'horrible'}]\n","pos {'tok_id': 2307, 'tok_str': 'great', 'loss': tensor(0.5285)}\n","neg {'tok_id': 2919, 'tok_str': 'bad', 'loss': tensor(0.5285)}\n","[Best epoch]\n","[Update verbalizer] ['bad', 'great']\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.52it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 1/10 | Train Loss: 0.5285 | Train time: 5.140798091888428s\n","[Eval] Acc:0.799 | pred.sum: 495 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4/4 [00:01<00:00,  3.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[{'tok_id': 3376, 'tok_str': 'beautiful'}, {'tok_id': 12047, 'tok_str': 'magnificent'}, {'tok_id': 6919, 'tok_str': 'wonderful'}, {'tok_id': 9487, 'tok_str': 'remarkable'}, {'tok_id': 2307, 'tok_str': 'great'}] [{'tok_id': 2919, 'tok_str': 'bad'}, {'tok_id': 6659, 'tok_str': 'terrible'}, {'tok_id': 6530, 'tok_str': 'dirty'}, {'tok_id': 9202, 'tok_str': 'horrible'}, {'tok_id': 5469, 'tok_str': 'horror'}]\n","pos {'tok_id': 2307, 'tok_str': 'great', 'loss': tensor(0.5042)}\n","neg {'tok_id': 2919, 'tok_str': 'bad', 'loss': tensor(0.5042)}\n","[Best epoch]\n","[Update verbalizer] ['bad', 'great']\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.55it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 2/10 | Train Loss: 0.5039 | Train time: 5.0692362785339355s\n","[Eval] Acc:0.799 | pred.sum: 421 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4/4 [00:01<00:00,  3.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[{'tok_id': 3376, 'tok_str': 'beautiful'}, {'tok_id': 9487, 'tok_str': 'remarkable'}, {'tok_id': 12047, 'tok_str': 'magnificent'}, {'tok_id': 2307, 'tok_str': 'great'}, {'tok_id': 6919, 'tok_str': 'wonderful'}] [{'tok_id': 2919, 'tok_str': 'bad'}, {'tok_id': 6659, 'tok_str': 'terrible'}, {'tok_id': 6530, 'tok_str': 'dirty'}, {'tok_id': 11083, 'tok_str': 'rotten'}, {'tok_id': 9202, 'tok_str': 'horrible'}]\n","pos {'tok_id': 2307, 'tok_str': 'great', 'loss': tensor(0.5034)}\n","neg {'tok_id': 2919, 'tok_str': 'bad', 'loss': tensor(0.5034)}\n","[Update verbalizer] ['bad', 'great']\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:04<00:00,  1.18s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.38it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 3/10 | Train Loss: 0.5038 | Train time: 4.963277339935303s\n","[Eval] Acc:0.796 | pred.sum: 406 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4/4 [00:01<00:00,  3.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[{'tok_id': 2307, 'tok_str': 'great'}, {'tok_id': 3376, 'tok_str': 'beautiful'}, {'tok_id': 12047, 'tok_str': 'magnificent'}, {'tok_id': 9487, 'tok_str': 'remarkable'}, {'tok_id': 6919, 'tok_str': 'wonderful'}] [{'tok_id': 2919, 'tok_str': 'bad'}, {'tok_id': 11083, 'tok_str': 'rotten'}, {'tok_id': 6530, 'tok_str': 'dirty'}, {'tok_id': 6659, 'tok_str': 'terrible'}, {'tok_id': 9202, 'tok_str': 'horrible'}]\n","pos {'tok_id': 2307, 'tok_str': 'great', 'loss': tensor(0.5033)}\n","neg {'tok_id': 2919, 'tok_str': 'bad', 'loss': tensor(0.5033)}\n","[Update verbalizer] ['bad', 'great']\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.45it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 4/10 | Train Loss: 0.5034 | Train time: 5.039231300354004s\n","[Eval] Acc:0.794 | pred.sum: 406 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4/4 [00:01<00:00,  3.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[{'tok_id': 2307, 'tok_str': 'great'}, {'tok_id': 12047, 'tok_str': 'magnificent'}, {'tok_id': 3376, 'tok_str': 'beautiful'}, {'tok_id': 9487, 'tok_str': 'remarkable'}, {'tok_id': 6919, 'tok_str': 'wonderful'}] [{'tok_id': 2919, 'tok_str': 'bad'}, {'tok_id': 6530, 'tok_str': 'dirty'}, {'tok_id': 11083, 'tok_str': 'rotten'}, {'tok_id': 9202, 'tok_str': 'horrible'}, {'tok_id': 3532, 'tok_str': 'poor'}]\n","pos {'tok_id': 2307, 'tok_str': 'great', 'loss': tensor(0.5032)}\n","neg {'tok_id': 2919, 'tok_str': 'bad', 'loss': tensor(0.5032)}\n","[WARNING] early stop at epoch 4.\n"]}],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["idx = 100\n","if idx >= 0:\n","    logits = fin_logits_eval[idx]\n","    pred = fin_outputs_eval[idx]\n","\n","    target = valid_dataset.target[idx]\n","    sequence = valid_dataset.getReview(idx)\n","    ids = valid_dataset[idx]['ids']\n","    print(f\"sequence: \\'{sequence}\\', target: {target}, pred: {pred}\", show_topk_cloze(logits, top_k=10))"],"outputs":[{"output_type":"stream","name":"stdout","text":["sequence: 'i was feeling this movie until it veered off too far into the exxon zone , and left me behind at the station looking for a return ticket to realism . It is a [MASK] film .', target: 0, pred: 0 [{'token_id': 2919, 'token_str': 'bad', 'raw_score': tensor(11.0937)}, {'token_id': 2439, 'token_str': 'lost', 'raw_score': tensor(10.8009)}, {'token_id': 6659, 'token_str': 'terrible', 'raw_score': tensor(9.2584)}, {'token_id': 4326, 'token_str': 'strange', 'raw_score': tensor(9.2245)}, {'token_id': 4689, 'token_str': 'crazy', 'raw_score': tensor(8.4266)}, {'token_id': 2757, 'token_str': 'dead', 'raw_score': tensor(8.0671)}, {'token_id': 9202, 'token_str': 'horrible', 'raw_score': tensor(7.9128)}, {'token_id': 3714, 'token_str': 'broken', 'raw_score': tensor(7.7774)}, {'token_id': 3974, 'token_str': 'losing', 'raw_score': tensor(7.7591)}, {'token_id': 5305, 'token_str': 'sick', 'raw_score': tensor(7.5389)}]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["# 1. 测试\n","eval_acc = max(ev_acc_his) # best eval\n","\n","if config.test_output:\n","    model = torch.load(f\"fewshot{config.few_shot}-{config.BERT_PATH}-best.pth\")\n","    _, test_dir= dataset.read_data(config.TEST_FILE, test=True)\n","    test_dataset = PromptDataset(test_dir['x'], test_dir['y'],config=config)\n","    test_data_loader = test_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","\n","    test_record = eval_prompt(test_data_loader, model, device)\n","    # targets ,outputs ,logits ,mask_ids\n","    test_preds = test_record[1]\n","\n","    # 2. open文件写入结果\n","    with open(f'saved/few_shot{config.few_shot}_eval{eval_acc}_res.txt',encoding=\"utf-8\", mode='w') as f:\n","        for pred in test_preds:\n","            f.write(\"positive\" if pred==1 else 'negative')\n","            f.write('\\n')\n","    print(\"Testing finish. Test results saved.\")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["metric_rec = {\n","    'epo':[(i+1)*config.train_times for i in range(len(ev_acc_his))],\n","    'eval acc': ev_acc_his,\n","    'train loss': tr_loss_his ,\n","    'epoch time(s)': tr_time_his\n","}\n","data_f = pd.DataFrame(metric_rec)\n","data_f"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epo</th>\n","      <th>eval acc</th>\n","      <th>train loss</th>\n","      <th>epoch time(s)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.777</td>\n","      <td>0.645327</td>\n","      <td>7.529954</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.799</td>\n","      <td>0.528524</td>\n","      <td>5.140798</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.799</td>\n","      <td>0.503862</td>\n","      <td>5.069236</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.796</td>\n","      <td>0.503775</td>\n","      <td>4.963277</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.794</td>\n","      <td>0.503368</td>\n","      <td>5.039231</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   epo  eval acc  train loss  epoch time(s)\n","0    1     0.777    0.645327       7.529954\n","1    2     0.799    0.528524       5.140798\n","2    3     0.799    0.503862       5.069236\n","3    4     0.796    0.503775       4.963277\n","4    5     0.794    0.503368       5.039231"]},"metadata":{},"execution_count":12}],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["avg_epo_time= np.average(tr_time_his)\n","print(\"model {} | fewshot {} | best acc {} | epoch {} | {:.3f}s\".format(config.BERT_PATH ,config.few_shot,eval_acc, config.EPOCHS,avg_epo_time))"],"outputs":[{"output_type":"stream","name":"stdout","text":["model bert-base-uncased | fewshot 32 | best acc 0.799 | epoch 10 | 5.548s\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}