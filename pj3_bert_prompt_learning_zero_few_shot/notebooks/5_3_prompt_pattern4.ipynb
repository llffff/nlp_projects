{"cells":[{"cell_type":"code","execution_count":1,"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\" \n","\n","from time import time\n","import random\n","\n","import dataset\n","from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import transformers\n","from  torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["class PromptConfig():\n","    def __init__(self, few_shot, BERT_PATH=\"bert-base-uncased\", ) -> None:\n","        \"\"\"\n","        few_shot=['0', '32', '64', '128']\n","        \"\"\"\n","        self.few_shot = few_shot\n","        self.DEVICE = \"cuda:3\"\n","        self.MAX_LEN = 256\n","        self.TRAIN_BATCH_SIZE = 8\n","        self.VALID_BATCH_SIZE = 4\n","        self.train_times = 1\n","        self.EPOCHS = 10*self.train_times\n","        \n","        self.EARLY_STOP = 3\n","        self.eval_zero_shot = False # 是否测试zero shot\n","        self.test_output = True # 是否测试输出\n","        self.UPDATE_VERBAL = False # 是否更新verbalizer\n","        self.use_demostration = True # 是否使用 prompt type4 demostration\n","        \n","        # 训练参数\n","        self.eps_thres=1e-4 \n","        self.es_max=5  # early stop\n","\n","        self.BERT_PATH = BERT_PATH\n","        self.MODEL_PATH = \"/home/18307110500/pj3_workplace/pytorch_model.bin\"\n","        data_dir =\"/home/18307110500/data\"\n","\n","        if few_shot is not None and few_shot == '0':\n","            self.TRAINING_FILE = None\n","        elif few_shot is not None and os.path.exists(f\"{data_dir}/train_{few_shot}.data\"):\n","            self.TRAINING_FILE =f\"{data_dir}/train_{few_shot}.data\"\n","        else:\n","            self.TRAINING_FILE = f\"{data_dir}/train.data\"\n","\n","        self.VALIDATION_FILE = f\"{data_dir}/valid.data\"\n","        self.TEST_FILE = f\"{data_dir}/test.data\"\n","\n","        BERT_PATH:str\n","        if BERT_PATH.startswith(\"bert\"):\n","            self.TOKENIZER= transformers.BertTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.BertForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        elif BERT_PATH.startswith(\"albert\"):\n","            # AlbertTokenizer, AlbertForMaskedLM\n","            self.TOKENIZER= transformers.AlbertTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.AlbertForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        elif BERT_PATH.startswith(\"roberta\"):\n","            # RobertaTokenizer, RobertaForMaskedLM\n","            self.TOKENIZER= transformers.RobertaTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.RobertaForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        # prompt\n","        # label转换为id\n","        self.mask = self.TOKENIZER.mask_token # '[MASK]'/'<mask>'\n","        # self.verbalizer=['negative', 'positive']\n","        self.verbalizer=['bad', 'great']\n","        self.template =  \"It is a {} film .\"\n","        # self.template =  \"It was {} .\" # .format('[MASK]')\n","        # self.verbalizer=['terrible', 'great']\n","        self.candidate_ids = [self.TOKENIZER._convert_token_to_id(_) for _ in self.verbalizer]\n","        \n","''' 基于 BertMaskedML的 few shot '''\n","paths= [\"bert-base-uncased\",\"bert-large-uncased\", \"albert-base-v2\", \"albert-large-v2\", \"roberta-base\",\"roberta-large\"]\n","config = PromptConfig(BERT_PATH=paths[0], few_shot=\"32\") # few shot"],"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["# model: bert masked lm\n","model_bert = config.MODEL \n","bert_tokenzier = config.TOKENIZER\n","\n","bert_tokenzier: transformers.BertTokenizer\n","\n","class PromptDataset(dataset.BERTDataset):\n","    def __init__(self, review, target, config):\n","        super(PromptDataset, self).__init__(review, target, config)\n","\n","        self.config = config\n","        self.config: PromptConfig\n","\n","        self.template = config.template # \"It is a {} film.\" # [MASK]\n","        self.mask = config.mask# '[MASK]' # bert_tokenzier.mask_token\n","        self.sep = config.TOKENIZER.sep_token\n","\n","        self.use_demostration = config.use_demostration\n","\n","    def _random_neg_pos(self, item):\n","        self_ = self\n","        item_i = item\n","\n","        break_epo = 50\n","        rand_ids = [-1,-1] #0: negative\n","        \n","        \n","        while not (rand_ids[0]>-1 and rand_ids[1]>-1) and break_epo > 0:\n","            break_epo -= 1 \n","            \n","            label_ = 0 if rand_ids[0] == -1 else 1 # 判断是找pos的随机样本还是neg\n","            \n","            rand_i = random.randint(0, len(self_)-1) # 样本随机值\n","            if rand_i == item_i: # 不能等于自身\n","                continue\n","            if not self_.target[rand_i] == label_: # 需要指定pos或neg\n","                continue\n","            \n","            if break_epo <= 0:\n","                rand_ids[label_] = label_\n","                continue\n","            \n","            rand_ids[label_] = rand_i\n","\n","        return rand_ids[0],rand_ids[1]\n","\n","    \n","    def make_prompt(self, input_data, replace=config.mask):\n","        input_trans = f\"{input_data} {self.template.format(replace)}\"\n","        return input_trans\n","        \n","\n","    def getReview(self, item):\n","        neg_item, pos_item = self._random_neg_pos(item)\n","\n","        # demonstration\n","        \n","        # 样本\n","        review = super().getReview(item)\n","        review_trans = self.make_prompt(review)\n","\n","        if not self.use_demostration:\n","            return review_trans\n","        \n","        # 随机负例\n","        review_neg = super().getReview(neg_item)\n","        review_trans_neg = self.make_prompt(review_neg, replace=self.config.verbalizer[0])\n","        \n","        # 随机正例\n","        review_pos = super().getReview(pos_item)\n","        review_trans_pos = self.make_prompt(review_pos,replace=self.config.verbalizer[1])\n","\n","        # 随机正例先或负例先\n","        if random.randint(0,1) == 0:\n","            review_trans = f\"{review_trans} {self.sep} {review_trans_neg} {self.sep} {review_trans_pos}\"\n","        else:\n","            review_trans = f\"{review_trans} {self.sep} {review_trans_pos} {self.sep} {review_trans_neg}\"\n","        \n","        return review_trans\n","        \n","\n","_, train_dir= dataset.read_data(config.TRAINING_FILE)\n","_, valid_dir= dataset.read_data(config.VALIDATION_FILE)\n","\n","train_dataset = PromptDataset(train_dir['x'], train_dir['y'],config=config)\n","valid_dataset = PromptDataset(valid_dir['x'], valid_dir['y'],config=config)\n","\n","valid_data_loader = valid_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","train_data_loader = train_dataset.get_dataloader(batch_size=config.TRAIN_BATCH_SIZE)\n","\n","print(train_dataset.getReview(0), train_dataset.target[0])\n","print(valid_dataset.getReview(0), valid_dataset.target[0])\n","# \"nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic .  It is a [MASK] review.\" 0"],"outputs":[{"output_type":"stream","name":"stdout","text":["# samples: 32\n","# samples: 1000\n","better than the tepid star trek : insurrection ; falls short of first contact because the villain could n't pick the lint off borg queen alice krige 's cape ; and finishes half a parsec ( a nose ) ahead of generations . It is a [MASK] film . [SEP] the powers team has fashioned a comedy with more laughs than many , no question . but this time there 's some mold on the gold . It is a great film . [SEP] build some robots , haul 'em to the theatre with you for the late show , and put on your own mystery science theatre 3000 tribute to what is almost certainly going to go down as the worst -- and only -- killer website movie of this or any other year . It is a bad film . 1\n","nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic . It is a [MASK] film . [SEP] it 's as raw and action-packed an experience as a ringside seat at a tough-man contest . It is a great film . [SEP] the movie 's heavy-handed screenplay navigates a fast fade into pomposity and pretentiousness . It is a bad film . 0\n"]}],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["def get_logits_of_mask(input_ids, logits, tok=config.mask, tokenzier=bert_tokenzier):\n","    \"\"\"\n","    Args:\n","        inputs_tok (tensor): 输入字符串经过tokenized得到的字典\n","        tok (str, optional): 可以是'[MASK]'或任意word token. Defaults to '[MASK]'.\n","\n","    Returns:\n","        (tensor, tensor): 返回mask处的logits，返回mask的列索引\n","\n","    Tips: 可以传入多个batch size\n","\n","    Modify: 改为torch实现\n","\n","    \"\"\"\n","    # find_idx_of_tok_in_seq\n","    tok_id =  tokenzier._convert_token_to_id(tok)\n","    ids_of_mask_in_seq = torch.nonzero(input_ids == tok_id)[:,1] ## 得到mask的列索引\n","    \n","    # convert to tensor\n","    logits_tok = torch.stack([logits[idx, ids_of_mask_in_seq[idx],:]for idx in range(logits.size(0))])\n","\n","    # logits_tok.size() # [4, 30522]=[batch size, vocab size]\n","    return logits_tok, ids_of_mask_in_seq\n","\n","\n","''' train: fine tune bert '''\n","\n","def count_acc(pred, target):\n","    acc_count = np.sum(np.array(pred) == np.array(target))\n","    return acc_count/len(pred)\n","\n","def loss_fn(outputs, targets):\n","    # sigmoid + cross entropy\n","    # print(outputs, targets)\n","    return nn.BCEWithLogitsLoss()(outputs.view(-1,1), targets.view(-1, 1))\n","\n","\n","def get_logits(config, logits_mask):\n","    # init: candidate_ids = config.candidate_ids\n","    labels_pr = logits_mask[:, config.candidate_ids]\n","    return labels_pr\n","\n","\n","def show_topk_cloze(logits_mask,top_k =10):\n","    # 根据logits排序\n","    top_inds = list(reversed(np.argsort(logits_mask)))\n","    res_top_k = []\n","    for i in top_inds:\n","        res_i = {\n","            \"token_id\":i.item(),\n","            \"token_str\": bert_tokenzier._convert_id_to_token(i.item()),\n","            \"raw_score\": logits_mask[i.item()] # 未经过softmax的分数\n","            }\n","        res_top_k.append(res_i)\n","        if len(res_top_k) >= top_k:\n","            break\n","\n","    return res_top_k # 查看top k预测的填空"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["def eval_prompt(data_loader, model, device):\n","    _targets = []\n","    _outputs = []\n","    _logits = []\n","    _mask_ids = []\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            \n","            input_token = {\n","                k[1]: d[k[0]].to(device) \n","                for k in dict_keys}\n","\n","            res_batch = model(**input_token)\n","            \n","            logits = res_batch.logits\n","            logits = logits.cpu()\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok=config.mask ,tokenzier=bert_tokenzier)\n","\n","            # optional: 计算整个vocab上的softmax score\n","            # logits_mask = torch.softmax(logits_mask, dim=-1)\n","            \n","            # 取出verbalizer对应的logits\n","            labels_pr = get_logits(config, logits_mask)\n","            pred = [np.argmax(_) for _ in labels_pr]\n","                \n","            _targets.extend(d['targets'])\n","            _outputs.extend(pred)\n","            _logits.extend(logits_mask)\n","            _mask_ids.extend(mask_ids)\n","            torch.cuda.empty_cache()\n","            # break\n","    return _targets,_outputs,_logits,_mask_ids"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["device = config.DEVICE\n","model_bert.to(device)\n","\n","# zero\n","if config.eval_zero_shot:\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    \n","    print(\"[Zero shot]\")\n","    print(f\"[Eval] Acc:{count_acc(fin_outputs_eval,fin_targets_eval)} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["param_optimizer = list(model_bert.named_parameters())\n","no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","# for _,__ in param_optimizer:\n","#     print(_)\n","optimizer_parameters = [ {\n","        \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.001,\n","    },\n","        {\"params\": [ p for n, p in param_optimizer if any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","\n","print(\"opt param: \",len(optimizer_parameters[0]['params']))\n","print(\"no opt\",len(optimizer_parameters[1]['params']))"],"outputs":[{"output_type":"stream","name":"stdout","text":["opt param:  76\n","no opt 126\n"]}],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["num_train_steps = int(len(train_dir['x']) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n","if len(optimizer_parameters[0]['params']) > 20:\n","    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","else:\n","    # albert\n","    optimizer = AdamW(model_bert.parameters(), lr=3e-5)\n","    \n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["ev_acc_his = []\n","tr_loss_his = []\n","tr_time_his=[]\n","\n","early_stop_count = 0\n","\n","for epoch in range(config.EPOCHS//config.train_times):\n","        \n","    # begin training\n","    model_bert.train()\n","    tr_time_s = time()\n","\n","    tr_loss = []\n","\n","    # config.train_times = 5\n","    for epo_tr in range(config.train_times):\n","        for bi, d in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            targets = d['targets']\n","            input_token = {\n","                    k[1]: d[k[0]].to(device) \n","                    for k in dict_keys}\n","\n","            optimizer.zero_grad()\n","            res_batch = model_bert(**input_token)\n","            logits = res_batch.logits.cpu()\n","\n","            ''' 取出mask位置上，candidate label对应的logits '''\n","            # mask 位置的预测logits\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok=config.mask,tokenzier=bert_tokenzier)\n","            # logits_mask: (batch_size, vocab_size)\n","\n","            ### 计算loss\n","            \n","            # 取出verbalizer对应的logits\n","            labels_pr = get_logits(config, logits_mask)\n","            # 概率分数\n","            labels_pr = torch.softmax(labels_pr, dim=-1)\n","            # labels_pr: (batch_size, 2)\n","            # 取出 positive 对应的分数 (negative = 1-positive)\n","            pred = labels_pr[:,1]\n","            loss = loss_fn(pred, targets)\n","            \n","            tr_loss.append(loss.cpu().detach().item())\n","            # print(loss) # 0.6433\n","\n","            ### 反传\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            torch.cuda.empty_cache()\n","\n","    tr_time_his.append((time()-tr_time_s)/config.train_times)\n","    tr_loss_his.append(np.mean(tr_loss))\n","\n","    # begin eval\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    \n","    ev_acc_his.append(count_acc(fin_outputs_eval,fin_targets_eval))\n","\n","    loss_str = \"{:.4f}\".format(tr_loss_his[-1])\n","    print(f\"[Train] Epoch: {epoch}/{config.EPOCHS} | Train Loss: {loss_str} | Train time: {tr_time_his[-1]}s\")\n","    print(f\"[Eval] Acc:{ev_acc_his[-1]} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")\n","\n","    best_acc = max(ev_acc_his[:-1]) if epoch > 1 else -10\n","    if ev_acc_his[-1] > best_acc: # > best acc\n","        torch.save(model_bert, f\"fewshot{config.few_shot}-{config.BERT_PATH}-best.pth\")\n","        print(\"[Best epoch]\")\n","        # reset\n","        early_stop_count= 0\n","    if early_stop_count > config.EARLY_STOP: \n","        print(f\"[WARNING] early stop at epoch {epoch}.\")\n","        break"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:07<00:00,  1.87s/it]\n","100%|██████████| 250/250 [00:34<00:00,  7.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 0/10 | Train Loss: 0.6826 | Train time: 7.680593490600586s\n","[Eval] Acc:0.49 | pred.sum: 0 | target.sum: 510.0\n","[Best epoch]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 1/10 | Train Loss: 0.6788 | Train time: 5.143148899078369s\n","[Eval] Acc:0.551 | pred.sum: 85 | target.sum: 510.0\n","[Best epoch]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:05<00:00,  1.28s/it]\n","100%|██████████| 250/250 [00:32<00:00,  7.61it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 2/10 | Train Loss: 0.6894 | Train time: 5.366856813430786s\n","[Eval] Acc:0.492 | pred.sum: 2 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4/4 [00:05<00:00,  1.27s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.55it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 3/10 | Train Loss: 0.6748 | Train time: 5.3484275341033936s\n","[Eval] Acc:0.53 | pred.sum: 52 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4/4 [00:05<00:00,  1.28s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 4/10 | Train Loss: 0.6359 | Train time: 5.357529163360596s\n","[Eval] Acc:0.695 | pred.sum: 413 | target.sum: 510.0\n","[Best epoch]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n","100%|██████████| 250/250 [00:32<00:00,  7.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 5/10 | Train Loss: 0.6140 | Train time: 5.137706756591797s\n","[Eval] Acc:0.699 | pred.sum: 519 | target.sum: 510.0\n","[Best epoch]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:04<00:00,  1.16s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 6/10 | Train Loss: 0.6040 | Train time: 4.91191291809082s\n","[Eval] Acc:0.712 | pred.sum: 536 | target.sum: 510.0\n","[Best epoch]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:04<00:00,  1.16s/it]\n","100%|██████████| 250/250 [00:32<00:00,  7.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 7/10 | Train Loss: 0.5745 | Train time: 4.879376411437988s\n","[Eval] Acc:0.724 | pred.sum: 496 | target.sum: 510.0\n","[Best epoch]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:04<00:00,  1.25s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.56it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 8/10 | Train Loss: 0.5547 | Train time: 5.2522454261779785s\n","[Eval] Acc:0.72 | pred.sum: 570 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\n","100%|██████████| 250/250 [00:33<00:00,  7.52it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 9/10 | Train Loss: 0.5597 | Train time: 5.120924711227417s\n","[Eval] Acc:0.711 | pred.sum: 619 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["idx = 100\n","if idx >= 0:\n","    logits = fin_logits_eval[idx]\n","    pred = fin_outputs_eval[idx]\n","\n","    target = valid_dataset.target[idx]\n","    sequence = valid_dataset.getReview(idx)\n","    ids = valid_dataset[idx]['ids']\n","    print(f\"sequence: \\'{sequence}\\', target: {target}, pred: {pred}\", show_topk_cloze(logits, top_k=10))"],"outputs":[{"output_type":"stream","name":"stdout","text":["sequence: 'i was feeling this movie until it veered off too far into the exxon zone , and left me behind at the station looking for a return ticket to realism . It is a [MASK] film . [SEP] although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women . It is a great film . [SEP] what happened with pluto nash ? how did it ever get made ? It is a bad film .', target: 0, pred: 0 [{'token_id': 2919, 'token_str': 'bad', 'raw_score': tensor(11.4127)}, {'token_id': 2204, 'token_str': 'good', 'raw_score': tensor(11.0810)}, {'token_id': 2307, 'token_str': 'great', 'raw_score': tensor(10.0681)}, {'token_id': 3376, 'token_str': 'beautiful', 'raw_score': tensor(9.8282)}, {'token_id': 4326, 'token_str': 'strange', 'raw_score': tensor(9.2696)}, {'token_id': 6919, 'token_str': 'wonderful', 'raw_score': tensor(8.5589)}, {'token_id': 12459, 'token_str': 'scary', 'raw_score': tensor(8.2381)}, {'token_id': 6659, 'token_str': 'terrible', 'raw_score': tensor(7.9784)}, {'token_id': 10392, 'token_str': 'fantastic', 'raw_score': tensor(7.8533)}, {'token_id': 2986, 'token_str': 'fine', 'raw_score': tensor(7.6728)}]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["# 1. 测试\n","eval_acc = max(ev_acc_his) # best eval\n","\n","if config.test_output:\n","    model = torch.load(f\"fewshot{config.few_shot}-{config.BERT_PATH}-best.pth\")\n","    _, test_dir= dataset.read_data(config.TEST_FILE, test=True)\n","    test_dataset = PromptDataset(test_dir['x'], test_dir['y'],config=config)\n","    test_data_loader = test_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","\n","    test_record = eval_prompt(test_data_loader, model, device)\n","    # targets ,outputs ,logits ,mask_ids\n","    test_preds = test_record[1]\n","\n","    # 2. open文件写入结果\n","    with open(f'saved/few_shot{config.few_shot}_eval{eval_acc}_res.txt',encoding=\"utf-8\", mode='w') as f:\n","        for pred in test_preds:\n","            f.write(\"positive\" if pred==1 else 'negative')\n","            f.write('\\n')\n","    print(\"Testing finish. Test results saved.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["# samples: 1066\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 267/267 [00:35<00:00,  7.47it/s]"]},{"output_type":"stream","name":"stdout","text":["Testing finish. Test results saved.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["metric_rec = {\n","    'epo':[(i+1)*config.train_times for i in range(len(ev_acc_his))],\n","    'eval acc': ev_acc_his,\n","    'train loss': tr_loss_his ,\n","    'epoch time(s)': tr_time_his\n","}\n","data_f = pd.DataFrame(metric_rec)\n","data_f"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epo</th>\n","      <th>eval acc</th>\n","      <th>train loss</th>\n","      <th>epoch time(s)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.490</td>\n","      <td>0.682618</td>\n","      <td>7.680593</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.551</td>\n","      <td>0.678813</td>\n","      <td>5.143149</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.492</td>\n","      <td>0.689388</td>\n","      <td>5.366857</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.530</td>\n","      <td>0.674824</td>\n","      <td>5.348428</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.695</td>\n","      <td>0.635935</td>\n","      <td>5.357529</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>0.699</td>\n","      <td>0.614020</td>\n","      <td>5.137707</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>0.712</td>\n","      <td>0.603988</td>\n","      <td>4.911913</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>0.724</td>\n","      <td>0.574518</td>\n","      <td>4.879376</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>0.720</td>\n","      <td>0.554719</td>\n","      <td>5.252245</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>0.711</td>\n","      <td>0.559716</td>\n","      <td>5.120925</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   epo  eval acc  train loss  epoch time(s)\n","0    1     0.490    0.682618       7.680593\n","1    2     0.551    0.678813       5.143149\n","2    3     0.492    0.689388       5.366857\n","3    4     0.530    0.674824       5.348428\n","4    5     0.695    0.635935       5.357529\n","5    6     0.699    0.614020       5.137707\n","6    7     0.712    0.603988       4.911913\n","7    8     0.724    0.574518       4.879376\n","8    9     0.720    0.554719       5.252245\n","9   10     0.711    0.559716       5.120925"]},"metadata":{},"execution_count":12}],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["avg_epo_time= np.average(tr_time_his)\n","print(\"model {} | fewshot {} | best acc {} | epoch {} | {:.3f}s\".format(config.BERT_PATH ,config.few_shot,eval_acc, config.EPOCHS,avg_epo_time))"],"outputs":[{"output_type":"stream","name":"stdout","text":["model bert-base-uncased | fewshot 32 | best acc 0.724 | epoch 10 | 5.420s\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}