{"cells":[{"cell_type":"code","execution_count":1,"source":["import os\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\" # cuda:0, GPU1\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # cuda:0, GPU1\n","from time import time\n","\n","import dataset\n","from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import transformers\n","from  torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["class PromptConfig():\n","    def __init__(self, few_shot, BERT_PATH=\"bert-base-uncased\", ) -> None:\n","        \"\"\"\n","        few_shot=['0', '32', '64', '128']\n","        \"\"\"\n","        self.few_shot = few_shot\n","        self.DEVICE = \"cuda:0\"\n","        self.MAX_LEN = 256\n","        self.TRAIN_BATCH_SIZE = 8\n","        self.VALID_BATCH_SIZE = 4\n","        self.train_times = 2\n","        self.EPOCHS = 5*self.train_times\n","        \n","        self.EARLY_STOP = 3\n","        self.eval_zero_shot = True # 是否测试zero shot\n","        \n","        # 训练参数\n","        self.eps_thres=1e-4 \n","        self.es_max=5  # early stop\n","\n","        self.BERT_PATH = BERT_PATH\n","        self.MODEL_PATH = \"/home/18307110500/pj3_workplace/pytorch_model.bin\"\n","        data_dir =\"/home/18307110500/data\"\n","\n","        if few_shot is not None and few_shot == '0':\n","            self.TRAINING_FILE = None\n","        elif few_shot is not None and os.path.exists(f\"{data_dir}/train_{few_shot}.data\"):\n","            self.TRAINING_FILE =f\"{data_dir}/train_{few_shot}.data\"\n","        else:\n","            self.TRAINING_FILE = f\"{data_dir}/train.data\"\n","\n","        self.VALIDATION_FILE = f\"{data_dir}/valid.data\"\n","        self.TEST_FILE = f\"{data_dir}/test.data\"\n","\n","        BERT_PATH:str\n","        if BERT_PATH.startswith(\"bert\"):\n","            self.TOKENIZER= transformers.BertTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.BertForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        elif BERT_PATH.startswith(\"albert\"):\n","            # AlbertTokenizer, AlbertForMaskedLM\n","            self.TOKENIZER= transformers.AlbertTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.AlbertForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","        elif BERT_PATH.startswith(\"roberta\"):\n","            # RobertaTokenizer, RobertaForMaskedLM\n","            self.TOKENIZER= transformers.RobertaTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","            self.MODEL = transformers.RobertaForMaskedLM.from_pretrained(self.BERT_PATH)\n","            \n","            \n","        # prompt\n","        # label转换为id\n","        self.mask = self.TOKENIZER.mask_token # '[MASK]'/'<mask>'\n","        # self.verbalizer=['negative', 'positive']\n","        # self.verbalizer=['bad', 'great']\n","        # self.template =  \"It is a {} film .\"\n","        self.template =  \"It was {} .\" # .format('[MASK]')\n","        self.verbalizer=['terrible', 'great']\n","        self.candidate_ids = [self.TOKENIZER._convert_token_to_id(_) for _ in self.verbalizer]\n","        \n","''' 1  '''\n","''' 基于BertMaskedML的few shot '''\n","paths= [\"bert-base-uncased\",\"bert-large-uncased\", \"albert-base-v2\", \"albert-large-v2\", \"roberta-base\",\"roberta-large\"]\n","config = PromptConfig(BERT_PATH=paths[0], few_shot=\"32\") # few shot"],"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["# model: bert masked lm\n","model_bert = config.MODEL \n","bert_tokenzier = config.TOKENIZER\n","\n","bert_tokenzier: transformers.BertTokenizer\n","\n","class PromptDataset(dataset.BERTDataset):\n","    def __init__(self, review, target, config):\n","        super(PromptDataset, self).__init__(review, target, config)\n","\n","        self.template = config.template # \"It is a {} film.\" # [MASK]\n","        self.mask = config.mask# '[MASK]' # bert_tokenzier.mask_token\n","        \n","    # sep = bert_tokenzier.sep_token\n","    def make_prompt(self, input_data):\n","        input_trans = f\"{input_data} {self.template.format(self.mask)}\"\n","        \n","        return input_trans\n","\n","    def getReview(self, item):\n","        review = super().getReview(item)\n","        review_trans = self.make_prompt(review)\n","        return review_trans\n","        \n","\n","_, train_dir= dataset.read_data(config.TRAINING_FILE)\n","_, valid_dir= dataset.read_data(config.VALIDATION_FILE)\n","\n","train_dataset = PromptDataset(train_dir['x'], train_dir['y'],config=config)\n","valid_dataset = PromptDataset(valid_dir['x'], valid_dir['y'],config=config)\n","\n","valid_data_loader = valid_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","train_data_loader = train_dataset.get_dataloader(batch_size=config.TRAIN_BATCH_SIZE)\n","\n","print(train_dataset.getReview(0), train_dataset.target[0])\n","print(valid_dataset.getReview(0), valid_dataset.target[0])\n","# \"nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic .  It is a [MASK] review.\" 0"],"outputs":[{"output_type":"stream","name":"stdout","text":["# samples: 32\n","# samples: 1000\n","better than the tepid star trek : insurrection ; falls short of first contact because the villain could n't pick the lint off borg queen alice krige 's cape ; and finishes half a parsec ( a nose ) ahead of generations . It was [MASK] . 1\n","nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic . It was [MASK] . 0\n","\n","# samples: 1000\n","better than the tepid star trek : insurrection ; falls short of first contact because the villain could n't pick the lint off borg queen alice krige 's cape ; and finishes half a parsec ( a nose ) ahead of generations . It was [MASK] . 1\n","nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic . It was [MASK] . 0\n"]}],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["def get_logits_of_mask(input_ids,logits, tok='[MASK]', tokenzier=bert_tokenzier):\n","    \"\"\"\n","    Args:\n","        inputs_tok (tensor): 输入字符串经过tokenized得到的字典\n","        tok (str, optional): 可以是'[MASK]'或任意word token. Defaults to '[MASK]'.\n","\n","    Returns:\n","        (tensor, tensor): 返回mask处的logits，返回mask的列索引\n","\n","    Tips: 可以传入多个batch size\n","\n","    Modify: 改为torch实现\n","\n","    \"\"\"\n","\n","    # find_idx_of_tok_in_seq\n","    tok_id =  tokenzier._convert_token_to_id(tok)\n","    ids_of_mask_in_seq = torch.nonzero(input_ids == tok_id)[:,1] ## 得到mask的列索引\n","    \n","    # convert to tensor\n","    logits_tok = torch.stack([logits[idx, ids_of_mask_in_seq[idx],:]for idx in range(logits.size(0))])\n","\n","    # logits_tok.size() # [4, 30522]=[batch size, vocab size]\n","    return logits_tok, ids_of_mask_in_seq\n","\n","\n","''' train: fine tune bert '''\n","\n","def count_acc(pred, target):\n","    acc_count = np.sum(np.array(pred) == np.array(target))\n","    return acc_count/len(pred)\n","\n","def loss_fn(outputs, targets):\n","    # sigmoid + cross entropy\n","    # print(outputs, targets)\n","    return nn.BCEWithLogitsLoss()(outputs.view(-1,1), targets.view(-1, 1))\n","\n","\n","def get_logits(config, logits_mask):\n","    # init: candidate_ids = config.candidate_ids\n","    labels_pr = logits_mask[:, config.candidate_ids]\n","    return labels_pr\n","\n","def get_topk_token_ids(logits_mask,top_k =10):\n","    logits_mask_=logits_mask.detach()\n","    batch_size = logits_mask_.size(0)\n","    idsk = []\n","    logitsk = []\n","    \n","    for i in range(batch_size):\n","        \n","        top_inds = list(reversed(np.argsort(logits_mask_[i].numpy(), axis=-1)))  # list\n","        idsk.append(top_inds[:top_k])\n","\n","        logitsk.append(logits_mask_[i,top_inds][:top_k].numpy().tolist())\n","        \n","    return idsk, logitsk # (list, list) size=(bz, k)\n","\n","        \n","def show_topk_cloze(logits_mask,top_k =10):\n","    # 根据logits排序\n","    top_inds = list(reversed(np.argsort(logits_mask)))\n","    res_top_k = []\n","    for i in top_inds:\n","        res_i = {\n","            \"token_id\":i.item(),\n","            \"token_str\": bert_tokenzier._convert_id_to_token(i.item()),\n","            \"raw_score\": logits_mask[i.item()] # 未经过softmax的分数\n","            }\n","        res_top_k.append(res_i)\n","        if len(res_top_k) >= top_k:\n","            break\n","\n","    return res_top_k # 查看top k预测的填空"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["device = config.DEVICE\n","model_bert.to(device)\n","\n","param_optimizer = list(model_bert.named_parameters())\n","no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","# for _,__ in param_optimizer:\n","#     print(_)\n","\n","optimizer_parameters = [ {\n","        \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.001,\n","    },\n","        {\"params\": [ p for n, p in param_optimizer if any(nd in n for nd in no_decay) ],\n","        \"weight_decay\": 0.0,\n","    },\n","]\n","\n","print(\"opt param: \",len(optimizer_parameters[0]['params']))\n","print(\"no opt\",len(optimizer_parameters[1]['params']))"],"outputs":[{"output_type":"stream","name":"stdout","text":["opt param:  76\n","no opt 126\n"]}],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["num_train_steps = int(len(train_dir['x']) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n","if len(optimizer_parameters[0]['params']) > 20:\n","    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","else:\n","    # albert\n","    optimizer = AdamW(model_bert.parameters(), lr=3e-5)\n","    \n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["def eval_prompt(data_loader, model, device):\n","    _targets = []\n","    _outputs = []\n","    _logits = []\n","    _mask_ids = []\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            \n","            input_token = {\n","                k[1]: d[k[0]].to(device) \n","                for k in dict_keys}\n","\n","            res_batch = model(**input_token)\n","            \n","            logits = res_batch.logits\n","            logits = logits.cpu()\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok=config.mask,tokenzier=bert_tokenzier)\n","\n","            # optional: 计算整个vocab上的softmax score\n","            # logits_mask = torch.softmax(logits_mask, dim=-1)\n","            \n","            # 取出verbalizer对应的logits\n","            labels_pr = get_logits(config, logits_mask)\n","            pred = [np.argmax(_) for _ in labels_pr]\n","                \n","            _targets.extend(d['targets'])\n","            _outputs.extend(pred)\n","            _logits.extend(logits_mask)\n","            _mask_ids.extend(mask_ids)\n","            torch.cuda.empty_cache()\n","            # break\n","    return _targets,_outputs,_logits,_mask_ids\n","\n","\n","# zero\n","if config.eval_zero_shot:\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    \n","    print(\"[Zero shot]\")\n","    print(f\"[Eval] Acc:{count_acc(fin_outputs_eval,fin_targets_eval)} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 250/250 [00:41<00:00,  6.00it/s]"]},{"output_type":"stream","name":"stdout","text":["[Zero shot]\n","[Eval] Acc:0.582 | pred.sum: 864 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["def eval_prompt(data_loader, model, device):\n","    _targets = []\n","    _outputs = []\n","    _logits = []\n","    _mask_ids = []\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            \n","            input_token = {\n","                k[1]: d[k[0]].to(device) \n","                for k in dict_keys}\n","\n","            res_batch = model(**input_token)\n","            \n","            logits = res_batch.logits\n","            logits = logits.cpu()\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok=config.mask,tokenzier=bert_tokenzier)\n","\n","            # optional: 计算整个vocab上的softmax score\n","            # logits_mask = torch.softmax(logits_mask, dim=-1)\n","            \n","            # 取出verbalizer对应的logits\n","            labels_pr = get_logits(config, logits_mask)\n","            pred = [np.argmax(_) for _ in labels_pr]\n","                \n","            _targets.extend(d['targets'])\n","            _outputs.extend(pred)\n","            _logits.extend(logits_mask)\n","            _mask_ids.extend(mask_ids)\n","            torch.cuda.empty_cache()\n","            # break\n","    return _targets,_outputs,_logits,_mask_ids\n","\n","\n","# zero\n","if config.eval_zero_shot:\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    \n","    print(\"[Zero shot]\")\n","    print(f\"[Eval] Acc:{count_acc(fin_outputs_eval,fin_targets_eval)} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 250/250 [00:37<00:00,  6.67it/s]"]},{"output_type":"stream","name":"stdout","text":["[Zero shot]\n","[Eval] Acc:0.582 | pred.sum: 864 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["ev_acc_his = []\n","tr_loss_his = []\n","tr_time_his=[]\n","\n","early_stop_count = 0\n","\n","#test\n","# config.EPOCHS=config.train_times=1\n","# config.UPDATE_VERBAL = True\n","#test\n","\n","for epoch in range(config.EPOCHS//config.train_times):\n","        \n","    # begin training\n","    model_bert.train()\n","    tr_time_s = time()\n","\n","    tr_loss = []\n","    tr_topk_his=[]\n","    tr_targets_his=[]\n","\n","    # config.train_times = 5\n","    for epo_tr in range(config.train_times):\n","        for bi, d in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            targets = d['targets']\n","            input_token = {\n","                    k[1]: d[k[0]].to(device) \n","                    for k in dict_keys}\n","\n","            optimizer.zero_grad()\n","            res_batch = model_bert(**input_token)\n","            logits = res_batch.logits.cpu()\n","\n","            ''' 取出mask位置上，candidate label对应的logits '''\n","            # mask 位置的预测logits\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok=config.mask,tokenzier=bert_tokenzier)\n","            # logits_mask: (batch_size, vocab_size)\n","\n","            \n","            # 记录loss最小的top k verbalizer（不更新）\n","            # todo            \n","            topk_score, topk_ids = get_topk_token_ids(logits_mask)\n","            tr_topk_his.extend(topk_ids) # (batch size, topk)\n","            tr_targets_his.extend(targets) # (batch size, topk)\n","\n","            # 取出verbalizer对应的logits\n","            labels_pr = get_logits(config, logits_mask)\n","            # 概率分数\n","            labels_pr = torch.softmax(labels_pr, dim=-1)\n","            # labels_pr: (batch_size, 2)\n","\n","            # 取出 positive 对应的分数 (negative = 1-positive)\n","            pred = labels_pr[:,1]\n","            loss = loss_fn(pred, targets)\n","            tr_loss.append(loss.cpu().detach().item())\n","            # print(loss) # 0.6433\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            torch.cuda.empty_cache()\n","\n","    tr_time_his.append((time()-tr_time_s)/config.train_times)\n","    tr_loss_his.append(np.mean(tr_loss))\n","\n","    # begin eval\n","    fin_targets_eval,fin_outputs_eval,fin_logits_eval,fin_mask_ids_eval = eval_prompt(valid_data_loader, model_bert, device)\n","    ev_acc_his.append(count_acc(fin_outputs_eval,fin_targets_eval))\n","\n","    loss_str = \"{:.4f}\".format(tr_loss_his[-1])\n","    print(f\"[Train] Epoch: {epoch}/{config.EPOCHS} | Train Loss: {loss_str} | Train time: {tr_time_his[-1]}s\")\n","    print(f\"[Eval] Acc:{ev_acc_his[-1]} | pred.sum: {np.sum(fin_outputs_eval)} | target.sum: {np.sum(fin_targets_eval)}\")\n","\n","    best_acc = max(ev_acc_his[:-1]) if epoch > 1 else -10\n","    if ev_acc_his[-1] > best_acc: # > best acc\n","        torch.save(model_bert, f\"fewshot{config.few_shot}-{config.BERT_PATH}-best.pth\")\n","        print(\"[Best epoch]\")\n","        # reset\n","        early_stop_count= 0\n","    if early_stop_count > config.EARLY_STOP: \n","        print(f\"[WARNING] early stop at epoch {epoch}.\")\n","        break"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n","100%|██████████| 4/4 [00:05<00:00,  1.34s/it]\n","100%|██████████| 250/250 [00:35<00:00,  7.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 0/10 | Train Loss: 0.6246 | Train time: 5.501935005187988s\n","[Eval] Acc:0.708 | pred.sum: 602 | target.sum: 510.0\n","[Best epoch]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n","100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n","100%|██████████| 250/250 [00:35<00:00,  7.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 1/10 | Train Loss: 0.5108 | Train time: 5.564296126365662s\n","[Eval] Acc:0.755 | pred.sum: 479 | target.sum: 510.0\n","[Best epoch]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n","100%|██████████| 4/4 [00:05<00:00,  1.32s/it]\n","100%|██████████| 250/250 [00:35<00:00,  7.04it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 2/10 | Train Loss: 0.5035 | Train time: 5.476422309875488s\n","[Eval] Acc:0.755 | pred.sum: 583 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n","100%|██████████| 4/4 [00:05<00:00,  1.31s/it]\n","100%|██████████| 250/250 [00:35<00:00,  7.02it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 3/10 | Train Loss: 0.5034 | Train time: 5.441756725311279s\n","[Eval] Acc:0.745 | pred.sum: 609 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n","100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n","100%|██████████| 250/250 [00:35<00:00,  7.10it/s]"]},{"output_type":"stream","name":"stdout","text":["[Train] Epoch: 4/10 | Train Loss: 0.5034 | Train time: 5.439003586769104s\n","[Eval] Acc:0.746 | pred.sum: 608 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["idx = 100\n","if idx >= 0:\n","    logits = fin_logits_eval[idx]\n","    pred = fin_outputs_eval[idx]\n","\n","    target = valid_dataset.target[idx]\n","    sequence = valid_dataset.getReview(idx)\n","    ids = valid_dataset[idx]['ids']\n","    print(f\"sequence: \\'{sequence}\\', target: {target}, pred: {pred}\", show_topk_cloze(logits, top_k=10))"],"outputs":[{"output_type":"stream","name":"stdout","text":["sequence: 'i was feeling this movie until it veered off too far into the exxon zone , and left me behind at the station looking for a return ticket to realism . It was [MASK] .', target: 0, pred: 0 [{'token_id': 6659, 'token_str': 'terrible', 'raw_score': tensor(9.5199)}, {'token_id': 9643, 'token_str': 'awful', 'raw_score': tensor(9.0635)}, {'token_id': 9202, 'token_str': 'horrible', 'raw_score': tensor(8.8685)}, {'token_id': 20625, 'token_str': 'hopeless', 'raw_score': tensor(8.7770)}, {'token_id': 2058, 'token_str': 'over', 'raw_score': tensor(8.3041)}, {'token_id': 3109, 'token_str': 'hell', 'raw_score': tensor(7.5817)}, {'token_id': 5263, 'token_str': 'impossible', 'raw_score': tensor(7.5513)}, {'token_id': 2439, 'token_str': 'lost', 'raw_score': tensor(7.4029)}, {'token_id': 4326, 'token_str': 'strange', 'raw_score': tensor(7.3589)}, {'token_id': 4689, 'token_str': 'crazy', 'raw_score': tensor(7.3548)}]\n"]}],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["# 1. 测试\n","eval_acc = max(ev_acc_his) # best eval\n","model = torch.load(f\"fewshot{config.few_shot}-{config.BERT_PATH}-best.pth\")\n","\n","_, test_dir= dataset.read_data(config.TEST_FILE, test=True)\n","test_dataset = PromptDataset(test_dir['x'], test_dir['y'],config=config)\n","test_data_loader = test_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","\n","test_record = eval_prompt(test_data_loader, model, device)\n","# targets ,outputs ,logits ,mask_ids\n","test_preds = test_record[1]\n","\n","# 2. open文件写入结果\n","with open(f'saved/few_shot{config.few_shot}_eval{eval_acc}_res.txt',encoding=\"utf-8\", mode='w') as f:\n","    for pred in test_preds:\n","        f.write(\"positive\" if pred==1 else 'negative')\n","        f.write('\\n')\n","print(\"Testing finish. Test results saved.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["# samples: 1066\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 267/267 [00:38<00:00,  6.92it/s]"]},{"output_type":"stream","name":"stdout","text":["Testing finish. Test results saved.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["metric_rec = {\n","    'epo':[(i+1)*config.train_times for i in range(len(ev_acc_his))],\n","    'eval acc': ev_acc_his,\n","    'train loss': tr_loss_his ,\n","    'epoch time(s)': tr_time_his\n","}\n","data_f = pd.DataFrame(metric_rec)\n","data_f"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epo</th>\n","      <th>eval acc</th>\n","      <th>train loss</th>\n","      <th>epoch time(s)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>0.708</td>\n","      <td>0.624559</td>\n","      <td>5.501935</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>0.755</td>\n","      <td>0.510833</td>\n","      <td>5.564296</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6</td>\n","      <td>0.755</td>\n","      <td>0.503512</td>\n","      <td>5.476422</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>8</td>\n","      <td>0.745</td>\n","      <td>0.503426</td>\n","      <td>5.441757</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10</td>\n","      <td>0.746</td>\n","      <td>0.503385</td>\n","      <td>5.439004</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   epo  eval acc  train loss  epoch time(s)\n","0    2     0.708    0.624559       5.501935\n","1    4     0.755    0.510833       5.564296\n","2    6     0.755    0.503512       5.476422\n","3    8     0.745    0.503426       5.441757\n","4   10     0.746    0.503385       5.439004"]},"metadata":{},"execution_count":11}],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["avg_epo_time= np.average(tr_time_his)\n","print(\"model {} | fewshot {} | best acc {} | epoch {} | {:.3f}s\".format(config.BERT_PATH ,config.few_shot,eval_acc, config.EPOCHS,avg_epo_time))"],"outputs":[{"output_type":"stream","name":"stdout","text":["model bert-base-uncased | fewshot 32 | best acc 0.755 | epoch 10 | 5.485s\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}