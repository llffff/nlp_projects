{"cells":[{"cell_type":"code","execution_count":1,"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n","\n","import dataset\n","from tqdm import tqdm\n","import torch\n","import numpy as np\n","import transformers\n","\n","\n","\n","\n","class PromptConfig():\n","    def __init__(self, few_shot, BERT_PATH=\"bert-base-uncased\", ) -> None:\n","        \"\"\"\n","        few_shot=['0', '32', '64', '128']\n","        \"\"\"\n","        self.few_shot = few_shot\n","        self.DEVICE = \"cuda:1\"\n","        self.MAX_LEN = 256\n","        self.TRAIN_BATCH_SIZE = 8\n","        self.VALID_BATCH_SIZE = 4\n","        self.EPOCHS = 5\n","        \n","        # 训练参数\n","        self.eps_thres=1e-4 \n","        self.es_max=5  # early stop\n","\n","        self.BERT_PATH = BERT_PATH\n","        self.MODEL_PATH = \"/home/18307110500/pj3_workplace/pytorch_model.bin\"\n","        data_dir =\"/home/18307110500/data\"\n","\n","        if few_shot is not None and few_shot == '0':\n","            self.TRAINING_FILE = None\n","        elif few_shot is not None and os.path.exists(f\"{data_dir}/train_{few_shot}.data\"):\n","            self.TRAINING_FILE =f\"{data_dir}/train_{few_shot}.data\"\n","        else:\n","            self.TRAINING_FILE = f\"{data_dir}/train.data\"\n","\n","        self.VALIDATION_FILE = f\"{data_dir}/valid.data\"\n","        self.TEST_FILE = f\"{data_dir}/test.data\"\n","        self.TOKENIZER= transformers.BertTokenizer.from_pretrained(self.BERT_PATH, do_lower_case=True)\n","\n","        # prompt\n","        # label转换为id\n","        # self.verbalizer=['negative', 'positive']\n","        self.verbalizer=['bad', 'great']\n","        self.candidate_ids = [self.TOKENIZER._convert_token_to_id(_) for _ in self.verbalizer]\n","\n","        self.template =  \"It is a {} film.\" # .format('[MASK]')\n","        self.mask = self.TOKENIZER.mask_token # '[MASK]'"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["''' 0  '''\n","''' 基于BertMaskedML的zero shot '''\n","config = PromptConfig(BERT_PATH=\"bert-base-uncased\", few_shot=\"0\") # zero shot\n","\n","# model: bert masked lm\n","model_bert = transformers.BertForMaskedLM.from_pretrained(config.BERT_PATH)\n","bert_tokenzier = config.TOKENIZER\n","\n","bert_tokenzier: transformers.BertTokenizer"],"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["class PromptDataset(dataset.BERTDataset):\n","    def __init__(self, review, target, config):\n","        super(PromptDataset, self).__init__(review, target, config)\n","\n","        self.template = config.template # \"It is a {} film.\" # [MASK]\n","        self.mask = config.mask# '[MASK]' # bert_tokenzier.mask_token\n","        \n","    # sep = bert_tokenzier.sep_token\n","    def make_prompt(self, input_data):\n","        input_trans = f\"{input_data} {self.template.format(self.mask)}\"\n","        return input_trans\n","\n","    \n","    \n","    def getReview(self, item):\n","        review = super().getReview(item)\n","        review_trans = self.make_prompt(review)\n","        return review_trans\n","        \n","\n","_, valid_dir= dataset.read_data(config.VALIDATION_FILE)\n","valid_dataset = PromptDataset(valid_dir['x'], valid_dir['y'],config=config)\n","valid_data_loader = valid_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)\n","\n","print(valid_dataset.getReview(0), valid_dataset.target[0])\n","# \"nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic .  It is a [MASK] review.\" 0\n","\n","\n","def get_logits_of_mask(input_ids,logits, tok='[MASK]', tokenzier=bert_tokenzier):\n","    \"\"\"\n","    Args:\n","        inputs_tok (tensor): 输入字符串经过tokenized得到的字典\n","        tok (str, optional): 可以是'[MASK]'或任意word token. Defaults to '[MASK]'.\n","\n","    Returns:\n","        (tensor, tensor): 返回mask处的logits，返回mask的列索引\n","\n","    Tips: 可以传入多个batch size\n","\n","    Modify: 改为torch实现\n","    \"\"\"\n","\n","    # find_idx_of_tok_in_seq\n","    tok_id =  tokenzier._convert_token_to_id(tok)\n","    ids_of_mask_in_seq = torch.nonzero(input_ids == tok_id)[:,1] ## 得到mask的列索引\n","    \n","    # convert to tensor\n","    logits_tok = torch.stack([logits[idx, ids_of_mask_in_seq[idx],:]for idx in range(logits.size(0))])\n","\n","    # logits_tok.size() # [4, 30522]=[batch size, vocab size]\n","    return logits_tok, ids_of_mask_in_seq\n","\n","\n","    \n","device = config.DEVICE\n","model_bert.to(device)\n","\n","def count_acc(pred, target):\n","    acc_count = np.sum(np.array(pred)== np.array(target))\n","    return acc_count/len(pred)\n","\n","\n","\n","def eval_prompt(data_loader, model, device):\n","    _targets = []\n","    _outputs = []\n","    _logits = []\n","    _mask_ids = []\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","            dict_keys = [(\"ids\",\"input_ids\" ),(\"token_type_ids\",\"token_type_ids\"),(\"mask\",\"attention_mask\")]\n","            \n","            input_token = {\n","                k[1]: d[k[0]].to(device) \n","                for k in dict_keys}\n","\n","            res_batch = model(**input_token)\n","            \n","            logits = res_batch.logits\n","            logits = logits.cpu()\n","            logits_mask, mask_ids = get_logits_of_mask(d['ids'], logits, tok='[MASK]',tokenzier=bert_tokenzier)\n","\n","            # optional: 计算整个vocab上的softmax score\n","            # logits_mask = torch.softmax(logits_mask, dim=-1)\n","            \n","            # 比较哪个label的mask填空可能性更大\n","            labels_pr = logits_mask[:, config.candidate_ids]\n","            pred = [np.argmax(_) for _ in labels_pr]\n","                \n","            _targets.extend(d['targets'])\n","            _outputs.extend(pred)\n","            _logits.extend(logits_mask)\n","            _mask_ids.extend(mask_ids)\n","            torch.cuda.empty_cache()\n","            # break\n","    return _targets,_outputs,_logits,_mask_ids\n","\n","\n","fin_targets,fin_outputs,fin_logits,fin_mask_ids = eval_prompt(valid_data_loader, model_bert, device)\n","print(f\"avg acc:{count_acc(fin_outputs,fin_targets)} | pred.sum: {np.sum(fin_outputs)} | target.sum: {np.sum(fin_targets)}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["# samples: 1000\n","nothing about the film -- with the possible exception of elizabeth hurley 's breasts -- is authentic . It is a [MASK] film. 0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 250/250 [00:32<00:00,  7.73it/s]"]},{"output_type":"stream","name":"stdout","text":["avg acc:0.668 | pred.sum: 654 | target.sum: 510.0\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["def show_topk_cloze(logits_mask,top_k =10):\n","    \n","    # 根据logits排序\n","    top_inds = list(reversed(np.argsort(logits_mask)))\n","    res_top_k = []\n","    for i in top_inds:\n","        res_i = {\n","            \"token_id\":i.item(),\n","            \"token_str\": bert_tokenzier._convert_id_to_token(i.item()),\n","            \"raw_score\": logits_mask[i.item()] # 未经过softmax的分数\n","            }\n","        res_top_k.append(res_i)\n","        if len(res_top_k) >= top_k:\n","            break\n","\n","    return res_top_k # 查看top k预测的填空"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["idx = 3\n","logits = fin_logits[idx]\n","pred = fin_outputs[idx]\n","\n","target = valid_dataset.target[idx]\n","sequence = valid_dataset.getReview(idx)\n","ids = valid_dataset[idx]['ids']\n","(f\"sequence: \\'{sequence}\\', target: {target}, pred: {pred}\", show_topk_cloze(logits, top_k=40))"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(\"sequence: 'this angst-ridden territory was covered earlier and much better in ordinary people . It is a [MASK] film.', target: 0, pred: 0\",\n"," [{'token_id': 2439, 'token_str': 'lost', 'raw_score': tensor(8.7598)},\n","  {'token_id': 4516, 'token_str': 'documentary', 'raw_score': tensor(7.9718)},\n","  {'token_id': 2204, 'token_str': 'good', 'raw_score': tensor(7.8390)},\n","  {'token_id': 2460, 'token_str': 'short', 'raw_score': tensor(7.6915)},\n","  {'token_id': 3376, 'token_str': 'beautiful', 'raw_score': tensor(7.5731)},\n","  {'token_id': 16046, 'token_str': 'bollywood', 'raw_score': tensor(7.2638)},\n","  {'token_id': 8754, 'token_str': 'cult', 'raw_score': tensor(7.0753)},\n","  {'token_id': 4333, 'token_str': 'silent', 'raw_score': tensor(7.0202)},\n","  {'token_id': 5469, 'token_str': 'horror', 'raw_score': tensor(7.0167)},\n","  {'token_id': 3444, 'token_str': 'feature', 'raw_score': tensor(6.7615)},\n","  {'token_id': 2919, 'token_str': 'bad', 'raw_score': tensor(6.7300)},\n","  {'token_id': 2759, 'token_str': 'popular', 'raw_score': tensor(6.7216)},\n","  {'token_id': 3439, 'token_str': 'historical', 'raw_score': tensor(6.6860)},\n","  {'token_id': 6298, 'token_str': 'romantic', 'raw_score': tensor(6.5649)},\n","  {'token_id': 10398, 'token_str': 'propaganda', 'raw_score': tensor(6.5620)},\n","  {'token_id': 6801,\n","   'token_str': 'controversial',\n","   'raw_score': tensor(6.5030)},\n","  {'token_id': 2155, 'token_str': 'family', 'raw_score': tensor(6.3192)},\n","  {'token_id': 4038, 'token_str': 'comedy', 'raw_score': tensor(6.2686)},\n","  {'token_id': 10392, 'token_str': 'fantastic', 'raw_score': tensor(6.2078)},\n","  {'token_id': 5913, 'token_str': 'fantasy', 'raw_score': tensor(6.1646)},\n","  {'token_id': 9269, 'token_str': 'hindi', 'raw_score': tensor(6.1547)},\n","  {'token_id': 2307, 'token_str': 'great', 'raw_score': tensor(6.0968)},\n","  {'token_id': 6057, 'token_str': 'funny', 'raw_score': tensor(6.0235)},\n","  {'token_id': 7071, 'token_str': 'disaster', 'raw_score': tensor(5.9205)},\n","  {'token_id': 8317,\n","   'token_str': 'psychological',\n","   'raw_score': tensor(5.8331)},\n","  {'token_id': 2576, 'token_str': 'political', 'raw_score': tensor(5.7981)},\n","  {'token_id': 3144, 'token_str': 'successful', 'raw_score': tensor(5.7827)},\n","  {'token_id': 6919, 'token_str': 'wonderful', 'raw_score': tensor(5.7781)},\n","  {'token_id': 17636, 'token_str': 'bilingual', 'raw_score': tensor(5.7747)},\n","  {'token_id': 3315, 'token_str': 'musical', 'raw_score': tensor(5.7116)},\n","  {'token_id': 16524, 'token_str': 'surreal', 'raw_score': tensor(5.6905)},\n","  {'token_id': 2986, 'token_str': 'fine', 'raw_score': tensor(5.6817)},\n","  {'token_id': 8403, 'token_str': 'lovely', 'raw_score': tensor(5.6354)},\n","  {'token_id': 9487, 'token_str': 'remarkable', 'raw_score': tensor(5.6022)},\n","  {'token_id': 2845, 'token_str': 'russian', 'raw_score': tensor(5.5904)},\n","  {'token_id': 16747,\n","   'token_str': 'biographical',\n","   'raw_score': tensor(5.5894)},\n","  {'token_id': 4438, 'token_str': 'classic', 'raw_score': tensor(5.5543)},\n","  {'token_id': 4310, 'token_str': 'unique', 'raw_score': tensor(5.5523)},\n","  {'token_id': 20067, 'token_str': 'gangster', 'raw_score': tensor(5.5489)},\n","  {'token_id': 2718, 'token_str': 'hit', 'raw_score': tensor(5.5394)}])"]},"metadata":{},"execution_count":5}],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["model = model_bert\n","\n","_, test_dir= dataset.read_data(config.TEST_FILE, test=True)\n","test_dataset = PromptDataset(test_dir['x'], test_dir['y'],config=config)\n","test_data_loader = test_dataset.get_dataloader(batch_size=config.VALID_BATCH_SIZE)"],"outputs":[{"output_type":"stream","name":"stdout","text":["# samples: 1066\n"]}],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["# 1. 测试\n","test_record = eval_prompt(test_data_loader, model, device)\n","# targets ,outputs ,logits ,mask_ids\n","test_preds = test_record[1]\n","\n","# 2. open文件写入结果\n","eval_acc = count_acc(fin_outputs,fin_targets)\n","with open(f'saved/few_shot{config.few_shot}_eval{eval_acc}_res.txt',encoding=\"utf-8\", mode='w') as f:\n","    for pred in test_preds:\n","        f.write(\"positive\" if pred==1 else 'negative')\n","        f.write('\\n')"],"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 267/267 [00:32<00:00,  8.20it/s]\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}